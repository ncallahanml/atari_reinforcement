{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df726742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install \"gymnasium[atari]\"\n",
    "# !python -m pip install \"gymnasium[accept-rom-license, atari]\"\n",
    "# !pip install shimmy\n",
    "# !pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567e4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "import ale_py\n",
    "import shimmy\n",
    "import joblib\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from gym import wrappers\n",
    "from typing import Callable\n",
    "from dataclass import dataclass\n",
    "from skimage.measure import block_reduce\n",
    "from IPython.display import clear_output\n",
    "from video_frame_cache import VideoFrameCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f14fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as ln"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b7597b",
   "metadata": {},
   "source": [
    "| **Value** | **Meaning** |\n",
    "|:---------:|:-----------:|\n",
    "| 0 | NOOP |\n",
    "| 1 | FIRE |\n",
    "| 2 | RIGHT |\n",
    "| 3 | LEFT |\n",
    "| 4 | RIGHTFIRE |\n",
    "| 5 | LEFTFIRE |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ead628",
   "metadata": {},
   "source": [
    "# Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28496867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_obs(obs):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(obs)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15faaca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(probs, i_since_r, timer_i, c=1, sigma=None, buffer=None):        \n",
    "    # autograd no inplace ops\n",
    "    if buffer is None:\n",
    "        buffer = timer_i // 2\n",
    "    n = len(probs)\n",
    "    if sigma is None:\n",
    "        sigma = 2 / n\n",
    "    noise = torch.normal(0., sigma, (1, n), requires_grad=True) # means, stds shared, size n\n",
    "    noise = noise - noise.mean()\n",
    "    \n",
    "    scale = c * i_since_r / (timer_i - buffer)\n",
    "\n",
    "    probs = probs + noise * scale\n",
    "    pmin = torch.min(probs)\n",
    "    if pmin < 0:\n",
    "        probs = probs - pmin\n",
    "        probs = probs / torch.sum(probs)\n",
    "        \n",
    "    return probs\n",
    "\n",
    "def balance_lr(probs, i_since_r, timer_i, beta=.5, buffer=None):\n",
    "#     2 : 'RIGHT'\n",
    "#     3 : 'LEFT'\n",
    "#     4 : 'RIGHTFIRE'\n",
    "#     5 : 'LEFTFIRE'\n",
    "    if i_since_r < timer_i / 4:\n",
    "        return probs\n",
    "    elif i_since_r < timer_i / 2:\n",
    "        alpha = .5\n",
    "    elif i_since_r < timer_i * 3 / 4:\n",
    "        alpha = .8\n",
    "    else:\n",
    "        alpha = .99\n",
    "\n",
    "    zero_probs = torch.zeros_like(probs, requires_grad=True)\n",
    "    zero_probs[0,2] = (probs[0,3] - probs[0,2])\n",
    "    zero_probs[0,3] = (probs[0,2] - probs[0,3])\n",
    "    \n",
    "    zero_probs[0,4] = (probs[0,5] - probs[0,4])\n",
    "    zero_probs[0,5] = (probs[0,4] - probs[0,5])\n",
    "    zero_probs = zero_probs * alpha * beta / 2\n",
    "    \n",
    "    probs = probs + zero_probs\n",
    "    with torch.no_grad():\n",
    "        assert torch.sum(probs).round(decimals=3) == 1, torch.sum(probs)\n",
    "    return probs\n",
    "\n",
    "def standardize(x):\n",
    "    eps = np.finfo(np.float64).eps.item()\n",
    "    x = (x - x.mean()) / (x.std() + eps)\n",
    "    return x\n",
    "\n",
    "def balance_all(probs, i_since_r, timer_i, beta=2):\n",
    "    probs = probs + 2 * i_since_r / timer_i\n",
    "    probs = softmax = nn.Softmax(dim=-1)(probs)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595db1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(ln.Module):\n",
    "    num_classes : int = 6\n",
    "    hidden_sizes : Sequence = (128, 64)\n",
    "    kernel_init : Callable = ln.initializers.glorot_normal\n",
    "\n",
    "    @ln.compact\n",
    "    def __call__(self, x, return_activations=False):\n",
    "        activations = []\n",
    "        for hidden_size in self.hidden_sizes:\n",
    "            x = ln.Dense(\n",
    "                hidden_size,\n",
    "                kernel_init=self.kernel_init\n",
    "            )(x)\n",
    "            activations.append(x)\n",
    "            x = jax.nn.swish(x)\n",
    "            activations.append(x)\n",
    "            \n",
    "        x = ln.Dense(\n",
    "            self.num_classes,\n",
    "            kernel_init=self.kernel_init\n",
    "        )(x)\n",
    "        x = jax.nn.sigmoid(x)\n",
    "        activations.append(x)\n",
    "        return x if not return_activations else (x, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cde6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class JAXReinforcementBase():\n",
    "    model : ln.Module\n",
    "    reward_state : RewardState\n",
    "    optimizer : Callable\n",
    "    obs_shape : tuple\n",
    "    xmin : int=26\n",
    "    xmax : int=196\n",
    "    ymin : int=10\n",
    "    ymax : int=144\n",
    "    downsample : str='horizontal'\n",
    "    timer_i : int=1000\n",
    "    corner_correct : bool=True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.episode_rewards = list()\n",
    "        self.episode_probss = list()\n",
    "        self.probss = list()\n",
    "        \n",
    "        self.key = jax.random.key()\n",
    "        key1, key2 = jax.random.split(self.key)\n",
    "        init_obs = jax.random.normal(key1, self.obs_shape)\n",
    "        init_x = self.pre_process(init_obs)\n",
    "        self.params = self.model.init(key2, init_x)\n",
    "        \n",
    "        self.opt_state = optimizer.init(self.params)\n",
    "        self.loss_grad_fn = jax.value_and_grad(self.loss)\n",
    "        return\n",
    "        \n",
    "    def step(self, obs, i_since_r):\n",
    "        x = self.pre_process(obs) - self.prev_obs\n",
    "        self.prev_obs = x\n",
    "        \n",
    "        model_probs = self.model.apply(self.params, x)\n",
    "        probs = self.process_probs(model_probs, i_since_r)\n",
    "        self.episode_probss.append(probs)\n",
    "\n",
    "        key = jax.random.split(self.key)\n",
    "        action = jnp.random.choice(key, actions, p=probs).item()   \n",
    "        \n",
    "        adj_reward = self.reward_state.step(\n",
    "            action, \n",
    "            probs, \n",
    "            reward, \n",
    "            info, \n",
    "            i_since_r, \n",
    "            timer_i=self.timer_i,\n",
    "        ) \n",
    "        return action\n",
    "        \n",
    "    def log_probss(self):\n",
    "        log_probss = jnp.log(jnp.concatenate(self.probss, axis=0))\n",
    "        return log_probss\n",
    "        \n",
    "    def show_layers(self):\n",
    "        jax.tree_util.tree_map(lambda x: x.shape, self.params)\n",
    "        return\n",
    "    \n",
    "    #maybe move concatenates into loss or batch backward?\n",
    "    @static_method\n",
    "    def loss(log_probss, discounted_rewards):\n",
    "        loss = jnp.dot(log_probss, discounted_rewards)\n",
    "        print('loss shape', loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    @static_method\n",
    "    @jax.jit\n",
    "    def _backward(params, opt_state, loss_grad_fn, log_probss, discounted_rewards):\n",
    "        loss_val, grads = loss_grad_fn(log_probss, discounted_rewards)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state\n",
    "    \n",
    "    def batch_backward(self):\n",
    "        log_probss = self.log_probss()\n",
    "        assert log_probss.ndim ==2, log_probss.shape\n",
    "        discounted_rewards = self.reward_state.discounted_rewards()\n",
    "        self.params, self.opt_state = self._backward(\n",
    "            self.params, \n",
    "            self.opt_state,\n",
    "            self.loss_grad_fn,\n",
    "            log_probss,\n",
    "            discounted_rewards,\n",
    "        )\n",
    "        \n",
    "        self.batch_reset()\n",
    "        gc.collect()\n",
    "        return self\n",
    "    \n",
    "    def save(self, bytes_path):\n",
    "        model_bytes = flax.serialization.to_bytes(self.params)\n",
    "        # could chunk this\n",
    "        with open(bytes_path, 'wb') as f:\n",
    "            f.write(model_bytes)\n",
    "        return\n",
    "    \n",
    "    def load(self, bytes_path):\n",
    "        with open(bytes_path, 'rb') as f:\n",
    "            model_bytes = f.read()\n",
    "            \n",
    "        self.params = flax.serialization.from_bytes(self.params, model_bytes)\n",
    "        return\n",
    "    \n",
    "    def episode_reset(self, truncated=False):\n",
    "        episode_reward = self.reward_state.episode_reset(truncated=truncated)\n",
    "        if not truncated:\n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            self.probss += self.episode_probss\n",
    "            \n",
    "        self.episode_probss.clear()\n",
    "        self.prev_obs = 0\n",
    "        return\n",
    "    \n",
    "    def batch_reset(self):\n",
    "        self.reward_state.batch_reset()\n",
    "        self.probss.clear()\n",
    "        return\n",
    "        \n",
    "    def pre_process(self, obs):\n",
    "        assert obs.shape == self.obs_shape\n",
    "        x = x[self.xmin:self.xmax,self.ymin:self.ymax]\n",
    "        \n",
    "        if selfdownsample == 'horizontal':\n",
    "            x = x[::2,:]\n",
    "        x[x == 144] = 0 # erase background (background type 1)\n",
    "        x[x == 109] = 0 # erase background (background type 2)\n",
    "        x[x != 0] = 1 # everything else to 1\n",
    "        print(np.unique(x))\n",
    "        assert len(np.unique(x)) == 2, np.unique(x)\n",
    "        \n",
    "        if self.downsample == 'max_pool':\n",
    "            # ideally downsampling would be done before changing values in place, but this way the background is ignored easily\n",
    "            x = block_reduce(x, (2, 2), np.amax)\n",
    "        \n",
    "        x = jnp.asarray(np.expand_dims(x.ravel().float(), axis=0))\n",
    "        return x\n",
    "    \n",
    "    def process_probs(\n",
    "        self,\n",
    "        probs,\n",
    "        i_since_r,\n",
    "    ):\n",
    "        truncated = i_since_r > timer_i\n",
    "\n",
    "        if corner_correct: # heavily biases agent from getting 'stuck' in corner\n",
    "            probs = add_noise(probs, i_since_r, timer_i)\n",
    "            probs = balance_lr(probs, i_since_r, timer_i)\n",
    "        else:\n",
    "            probs = balance_all(probs, i_since_r, timer_i)\n",
    "\n",
    "        if torch.round(torch.sum(probs), decimals=4) != 1:\n",
    "            raise ValueError('Probs do not sum to 1')\n",
    "\n",
    "        return probs, truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddd9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvIter():\n",
    "    def __init__(self, game_name, pre_fn, max_episodes=100, **make_kwargs):\n",
    "        self.env = gym.make(game_name, **make_kwargs)\n",
    "        self.max_episodes = max_episodes\n",
    "        self.n_episodes = -1\n",
    "        self.reset()\n",
    "\n",
    "        self.pre_fn = preprocess_fn\n",
    "        self.prev_obs = None\n",
    "        return\n",
    "    \n",
    "    def standard_step(self):\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        if reward > 0:\n",
    "            self.last_i = i\n",
    "            self.i_since_r = 0\n",
    "        else:\n",
    "            self.i_since_r += 1\n",
    "            \n",
    "        self.i += 1\n",
    "        return self.i, self.i_since_r, obs, reward, terminated, truncated, info\n",
    "        \n",
    "    def reset_step(self):\n",
    "        obs, info = self.env.reset()\n",
    "        return 0, 0, x, obs, 0, False, False, info\n",
    "    \n",
    "    def reset(self, truncated=False):\n",
    "        self.reset_ = True\n",
    "        if not truncated:\n",
    "            self.n_episodes += 1\n",
    "        self.i = 0\n",
    "        self.last_i = 0\n",
    "        return\n",
    "    \n",
    "    def iter_all(self):\n",
    "        while self.n_episodes <= self.max_episodes:\n",
    "            if self.reset_:\n",
    "                self.reset_ = False\n",
    "                yield self.reset_step()\n",
    "            else:\n",
    "                yield self.standard_step()\n",
    "        env.close()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a43762",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardState():\n",
    "    def __init__(self, gamma=.99, reward_dict=None):\n",
    "        self.gamma = gamma\n",
    "        if reward_dict is None:\n",
    "            reward_dict = {\n",
    "                'life_penalty' : 15,\n",
    "                'nofire_penalty' : .1,\n",
    "                'comeback_reward' : 10,\n",
    "            }\n",
    "            \n",
    "        self.reward_dict = reward_dict\n",
    "        self.reward_sum = 0\n",
    "        self.adj_reward_sum = 0\n",
    "        self.episode_rewards = list()\n",
    "        self.probss = list()\n",
    "        return\n",
    "    \n",
    "    def episode_reset(self, truncated=False):           \n",
    "        self.reward_sum = 0\n",
    "        self.adj_reward_sum = 0\n",
    "        self.prev_lives = 3\n",
    "        if not truncated:\n",
    "            self.batch_rewards += self.episode_rewards\n",
    "        self.episode_rewards.clear()\n",
    "        return episode_reward\n",
    "    \n",
    "    def batch_reset(self):\n",
    "        self.episode_reset()\n",
    "        self.batch_rewards.clear()\n",
    "        return\n",
    "    \n",
    "    def step(self, reward, *args, **kwargs):\n",
    "        adj_reward = self.modify(reward, *args, **kwargs)\n",
    "        self.reward_sum += reward\n",
    "        self.adj_reward_sum += adj_reward\n",
    "        self.rewards.append(adj_reward)\n",
    "        return adj_reward\n",
    "    \n",
    "    def modify(self, action, reward, info, i_since_r, timer_i=1000):\n",
    "        if info['lives'] < self.prev_lives:\n",
    "            reward += reward_dict['life_penalty']\n",
    "        if reward <= 0 and action in [1,4,5]:\n",
    "            reward += reward_dict['nofire_penalty']\n",
    "        if reward > 0 and i_since_r > timer_i / 2:\n",
    "            reward += reward_dict['comeback_reward']\n",
    "            \n",
    "        self.prev_lives = info['lives']\n",
    "        return reward\n",
    "    \n",
    "    @static_method\n",
    "    @numba.jit\n",
    "    def _discount_rewards(rewards, gamma):\n",
    "        running_add = 0\n",
    "        discounted_rewards = list()\n",
    "        for reward in reversed(rewards):\n",
    "            running_add = running_add * gamma + reward\n",
    "            discounted_rewards.append(running_add)\n",
    "            \n",
    "        discounted_rewards = jnp.asarray(list(reversed(discounted_rewards)))\n",
    "        discounted_rewards = discounted_rewards - discounted_rewards.mean()\n",
    "        discounted_rewards = discounted_rewards / discounted_rewards.std()\n",
    "        return discounted_rewards\n",
    "    \n",
    "    def discount_rewards(self):\n",
    "        discounted_rewards = self._discount_rewards(self.rewards, self.gamma)\n",
    "        self.rewards.clear()\n",
    "        return discounted_rewards\n",
    "    \n",
    "    # uses .dot with discounted rewards as a loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc401574",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dict = {\n",
    "    0 : 'NOOP',\n",
    "    1 : 'FIRE',\n",
    "    2 : 'RIGHT',\n",
    "    3 : 'LEFT',\n",
    "    4 : 'RIGHTFIRE',\n",
    "    5 : 'LEFTFIRE'\n",
    "}\n",
    "actions = sorted(action_dict)\n",
    "\n",
    "obs_shape = (210, 160)\n",
    "lr = .01\n",
    "corner_correct = False\n",
    "timer_i = 200 # number of iterations without reward before noise is intentionally greater than signal\n",
    "\n",
    "batch_size = 64\n",
    "max_i = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d5052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(\n",
    "    num_classes=len(actions),\n",
    ")\n",
    "\n",
    "reward_state = RewardState()\n",
    "\n",
    "base = JAXReinforcementBase(\n",
    "    model,\n",
    "    reward_state,\n",
    "    optax.adam(learning_rate=lr),\n",
    "    obs_shape,\n",
    "    corner_correct=corner_correct,\n",
    "    timer_i=timer_i,\n",
    ")\n",
    "\n",
    "video_cache = VideoFrameCache()\n",
    "\n",
    "# EnvIter should change so that preprocessing is only in base\n",
    "env_iter = EnvIter(\n",
    "    'ALE/DemonAttack-v5', \n",
    "    base.pre_process,\n",
    "    n_episodes=10,\n",
    "    obs_type='grayscale', \n",
    "    render_mode=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7f00dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, i_since_r, obs, reward, terminated, truncated, info in env_iter.iter_all(): \n",
    "    video_cache.cache_append(obs)\n",
    "    model_probs, probs, action, loop_truncated = base.step(x, i_since_r)\n",
    "    truncated = truncated or loop_truncated or i >= max_i\n",
    "    ######################################################\n",
    "        \n",
    "    if terminated: # an episode finished intentionally\n",
    "        print(f'\\nEpisode {episode_number} of {n_episodes}, Iterations : {i}, Reward : {reward_sum}       \\n\\n', end='\\r')\n",
    "        base.episode_reset(truncated=False)\n",
    "        if not env_iter.n_episodes % batch_size:           \n",
    "            base.batch_backward()\n",
    "    elif truncated: # an episode terminated unexpectedly, shouldn't maintain results\n",
    "        base.episode_reset(truncated=True)\n",
    "    else:\n",
    "        print(i, end='                          \\r')\n",
    "        \n",
    "    if terminated or truncated:\n",
    "        video_cache.finish(f'episode_vids/episode_{env_iter.n_episodes}.mp4')\n",
    "        env_iter.reset(truncated=truncated)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
