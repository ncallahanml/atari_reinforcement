{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df726742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install \"gymnasium[atari]\"\n",
    "# !python -m pip install \"gymnasium[accept-rom-license, atari]\"\n",
    "# !pip install shimmy\n",
    "# !pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567e4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "import ale_py\n",
    "import shimmy\n",
    "import joblib\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from gym import wrappers\n",
    "from skimage.measure import block_reduce\n",
    "from IPython.display import clear_output\n",
    "from video_frame_cache import VideoFrameCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f14fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as ln"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b7597b",
   "metadata": {},
   "source": [
    "| **Value** | **Meaning** |\n",
    "|:---------:|:-----------:|\n",
    "| 0 | NOOP |\n",
    "| 1 | FIRE |\n",
    "| 2 | RIGHT |\n",
    "| 3 | LEFT |\n",
    "| 4 | RIGHTFIRE |\n",
    "| 5 | LEFTFIRE |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ffbd68",
   "metadata": {},
   "source": [
    "# Run Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7513bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "render = False # render video output?\n",
    "show = False\n",
    "no_grad = False\n",
    "corner_correct = False\n",
    "plot_action_every = 0 # 64\n",
    "\n",
    "timer_i = 2000 # number of iterations without reward before noise is intentionally greater than signal\n",
    "dropout_i = 1000\n",
    "\n",
    "record_actions = False # will be handled as true if plot_action_every > 0\n",
    "record_probs = False\n",
    "record_rewards = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ead628",
   "metadata": {},
   "source": [
    "# Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe4b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, window_size) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[window_size:] = ret[window_size:] - ret[:-window_size]\n",
    "    return ret[window_size - 1:] / window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28496867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_obs(obs):\n",
    "    \"\"\" \n",
    "    Simple display of image observation \n",
    "    \n",
    "    Args:\n",
    "    `obs` : np.ndarray\n",
    "    - Observation from the environment\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(obs)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15faaca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(probs, i_since_r, timer_i, c=1, sigma=None, buffer=None):        \n",
    "    # autograd no inplace ops\n",
    "    if buffer is None:\n",
    "        buffer = timer_i // 2\n",
    "    n = len(probs)\n",
    "    if sigma is None:\n",
    "        sigma = 2 / n\n",
    "    noise = torch.normal(0., sigma, (1, n), requires_grad=True) # means, stds shared, size n\n",
    "    noise = noise - noise.mean()\n",
    "    \n",
    "    scale = c * i_since_r / (timer_i - buffer)\n",
    "\n",
    "    probs = probs + noise * scale\n",
    "    pmin = torch.min(probs)\n",
    "    if pmin < 0:\n",
    "        probs = probs - pmin\n",
    "        probs = probs / torch.sum(probs)\n",
    "        \n",
    "    return probs\n",
    "\n",
    "def balance_lr(probs, i_since_r, timer_i, beta=.5, buffer=None):\n",
    "#     2 : 'RIGHT'\n",
    "#     3 : 'LEFT'\n",
    "#     4 : 'RIGHTFIRE'\n",
    "#     5 : 'LEFTFIRE'\n",
    "    if i_since_r < timer_i / 4:\n",
    "        return probs\n",
    "    elif i_since_r < timer_i / 2:\n",
    "        alpha = .5\n",
    "    elif i_since_r < timer_i * 3 / 4:\n",
    "        alpha = .8\n",
    "    else:\n",
    "        alpha = .99\n",
    "\n",
    "    zero_probs = torch.zeros_like(probs, requires_grad=True)\n",
    "    zero_probs[0,2] = (probs[0,3] - probs[0,2])\n",
    "    zero_probs[0,3] = (probs[0,2] - probs[0,3])\n",
    "    \n",
    "    zero_probs[0,4] = (probs[0,5] - probs[0,4])\n",
    "    zero_probs[0,5] = (probs[0,4] - probs[0,5])\n",
    "    zero_probs = zero_probs * alpha * beta / 2\n",
    "    \n",
    "    probs = probs + zero_probs\n",
    "    with torch.no_grad():\n",
    "        assert torch.sum(probs).round(decimals=3) == 1, torch.sum(probs)\n",
    "    return probs\n",
    "\n",
    "def standardize(x):\n",
    "    eps = np.finfo(np.float64).eps.item()\n",
    "    x = (x - x.mean()) / (x.std() + eps)\n",
    "    return x\n",
    "\n",
    "def balance_all(probs, i_since_r, timer_i, beta=2):\n",
    "    probs = probs + 2 * i_since_r / timer_i\n",
    "    probs = softmax = nn.Softmax(dim=-1)(probs)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829d65ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_probs(probs, i_since_r, timer_i=1000, corner_correct=True):\n",
    "    truncated = i_since_r > timer_i\n",
    "        \n",
    "    if corner_correct: # heavily biases agent from getting 'stuck' in corner\n",
    "        probs = add_noise(probs, i_since_r, timer_i)\n",
    "        probs = balance_lr(probs, i_since_r, timer_i)\n",
    "    else:\n",
    "        probs = balance_all(probs, i_since_r, timer_i)\n",
    "\n",
    "    if torch.round(torch.sum(probs), decimals=4) != 1:\n",
    "        raise ValueError('Probs do not sum to 1')\n",
    "        \n",
    "    return probs, truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc47b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBS_SHAPE = (210, 160)\n",
    "action_dict = {\n",
    "    0 : 'NOOP',\n",
    "    1 : 'FIRE',\n",
    "    2 : 'RIGHT',\n",
    "    3 : 'LEFT',\n",
    "    4 : 'RIGHTFIRE',\n",
    "    5 : 'LEFTFIRE'\n",
    "}\n",
    "actions = sorted(action_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d315640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as ln\n",
    "\n",
    "class LinearModel(ln.Module):\n",
    "    num_classes : int = 6\n",
    "    hidden_sizes : Sequence = (512, 256, 256, 128)\n",
    "    kernel_init : Callable = nn.linear.default_kernel_init\n",
    "\n",
    "    @ln.compact\n",
    "    def __call__(self, x, return_activations=False):\n",
    "        activations = []\n",
    "        for hidden_size in self.hidden_sizes:\n",
    "            x = ln.Dense(\n",
    "                hidden_size,\n",
    "                kernel_init=self.kernel_init\n",
    "            )(x)\n",
    "            activations.append(x)\n",
    "            x = jax.nn.swish(x)\n",
    "            activations.append(x)\n",
    "            \n",
    "        x = ln.Dense(\n",
    "            self.num_classes,\n",
    "            kernel_init=self.kernel_init\n",
    "        )(x)\n",
    "        x = jax.nn.sigmoid(x)\n",
    "        activations.append(x)\n",
    "        return x if not return_activations else (x, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cde6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforcementBase(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        gamma=.99, \n",
    "        lr=.1,\n",
    "        xmin=26, \n",
    "        xmax=196, \n",
    "        ymin=10, \n",
    "        ymax=144,\n",
    "        downsample = 'horizontal',\n",
    "    ):\n",
    "        super(LinearReinforcement, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.xmin = xmin\n",
    "        self.xmax = xmax\n",
    "        self.ymin = ymin\n",
    "        self.ymax = ymax\n",
    "        self.downsample = downsample\n",
    "        \n",
    "        # preprocess args\n",
    "        input_shape = (xmax - xmin, ymax - ymin)\n",
    "\n",
    "        self.log_probss = list()\n",
    "        self.rewards = list()\n",
    "        self.episode_losses = list()\n",
    "        return\n",
    "\n",
    "    def episode_end(self):\n",
    "        assert len(self.log_probss)\n",
    "        episode_loss = self.reward_loss()\n",
    "        self.episode_losses.append(jnp.expand_dims(episode_loss, 0))\n",
    "        \n",
    "        self.log_probss.clear()\n",
    "        self.rewards.clear()\n",
    "        return self\n",
    "    \n",
    "    def batch_backward(self):\n",
    "        assert len(self.episode_losses)\n",
    "#         self.optimizer.zero_grad() # make sure this shouldn't go in reward_loss\n",
    "        loss = jnp.sum(self.episode_losses)\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "        self.episode_losses.clear()\n",
    "        return self\n",
    "    \n",
    "    def reset(self):\n",
    "        self.log_probss.clear()\n",
    "        self.rewards.clear()\n",
    "        return\n",
    "        \n",
    "    def preprocess(self, x):\n",
    "        assert x.shape == (210, 160)\n",
    "        x = x[self.xmin:self.xmax,self.ymin:self.ymax]\n",
    "        if self.downsample == 'horizontal':\n",
    "            x = x[::2,:]\n",
    "        x[x == 144] = 0 # erase background (background type 1)\n",
    "        x[x == 109] = 0 # erase background (background type 2)\n",
    "        x[x != 0] = 1 # everything else to 1\n",
    "        if self.downsample == 'max_pool':\n",
    "            # ideally downsampling would be done before changing values in place, but this way the background is ignored easily\n",
    "            x = block_reduce(x, (2, 2), np.amax)\n",
    "        x = jnp.asarray(x.ravel().float().expand_dims(axis=0))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc401574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "save_path = 'model.pt'\n",
    "always_collect = True\n",
    "clear = True\n",
    "resume = False # resume training from previous checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddd9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvIter():\n",
    "    def __init__(self, game_name, pre_fn, max_episodes=100, **make_kwargs):\n",
    "        self.env = gym.make(game_name, **make_kwargs)\n",
    "        self.max_episodes = max_episodes\n",
    "        self.n_episodes = -1\n",
    "        self.reset()\n",
    "\n",
    "        self.pre_fn = preprocess_fn\n",
    "        self.prev_obs = None\n",
    "        return\n",
    "    \n",
    "    def standard_step(self):\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        x = self.pre_fn(obs)\n",
    "        if self.prev_obs is None:\n",
    "            raise ValueError('Reset problem')\n",
    "        x = x - self.prev_obs\n",
    "        self.prev_obs = x\n",
    "        return x, obs, reward, terminated, truncated, info\n",
    "        \n",
    "    def reset_step(self):\n",
    "        obs, info = self.env.reset()\n",
    "        x = self.pre_fn(obs)\n",
    "        self.prev_obs = x\n",
    "        return x, obs, 0, False, False, info\n",
    "    \n",
    "    def reset(self):\n",
    "        self.reset_ = True\n",
    "        self.n_episodes += 1\n",
    "        return\n",
    "    \n",
    "    def iter_all(self):\n",
    "        while self.n_episodes <= self.max_episodes:\n",
    "            if self.reset_:\n",
    "                self.reset_ = False\n",
    "                yield self.reset_step()\n",
    "            else:\n",
    "                yield self.standard_step()\n",
    "        \n",
    "env_iter = EnvIter(\n",
    "    'ALE/DemonAttack-v5', \n",
    "    model.preprocess,\n",
    "    n_episodes=10,\n",
    "    obs_type='grayscale', \n",
    "    render_mode=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d5052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardState():\n",
    "    def __init__(self, reward_dict=None):\n",
    "        if reward_dict is None:\n",
    "            reward_dict = {\n",
    "                'life_penalty' : 15,\n",
    "                'nofire_penalty' : .1,\n",
    "                'comeback_reward' : 10,\n",
    "            }\n",
    "            \n",
    "        self.reward_dict = reward_dict\n",
    "        self.reward_sum = 0\n",
    "        self.adj_reward_sum = 0\n",
    "        self.rewards = list()\n",
    "        self.probss = list()\n",
    "        return\n",
    "    \n",
    "    def reset(self):\n",
    "        self.reward_sum = 0\n",
    "        self.adj_reward_sum = 0\n",
    "        self.prev_lives = 3\n",
    "        return\n",
    "    \n",
    "    def step(self, reward, probs, *args, **kwargs):\n",
    "        adj_reward = self.modify(reward, *args, **kwargs)\n",
    "        \n",
    "        self.reward_sum += reward\n",
    "        self.adj_reward_sum += adj_reward\n",
    "        self.probss.append(probs)\n",
    "        self.rewards.append(adj_reward)\n",
    "        return adj_reward\n",
    "    \n",
    "    def modify(self, action, reward, info, prev_lives, i_since_r, timer_i=1000):\n",
    "        if info['lives'] < self.prev_lives:\n",
    "            reward += reward_dict['life_penalty']\n",
    "        if reward <= 0 and action in [1,4,5]:\n",
    "            reward += reward_dict['nofire_penalty']\n",
    "        if reward > 0 and i_since_r > timer_i / 2:\n",
    "            reward += reward_dict['comeback_reward']\n",
    "            \n",
    "        self.prev_lives = info['lives']\n",
    "        return reward\n",
    "    \n",
    "    def discount_rewards(self):\n",
    "        running_add = 0\n",
    "        discounted_rewards = list()\n",
    "        for reward in reversed(self.rewards):\n",
    "            running_add = running_add * self.gamma + reward\n",
    "            discounted_rewards.append(running_add)\n",
    "            \n",
    "        discounted_rewards = jnp.asarray(list(reversed(discounted_rewards)))\n",
    "        discounted_rewards = discounted_rewards - discounted_rewards.mean()\n",
    "        discounted_rewards = discounted_rewards / discounted_rewards.std()\n",
    "        self.rewards.clear()\n",
    "        return discounted_rewards\n",
    "    \n",
    "    def log_probss(self):\n",
    "        log_probss = jnp.log(jnp.concatenate(self.probs, axis=0))\n",
    "        self.probs.clear()\n",
    "        return log_probss\n",
    "    # uses .dot with discounted rewards as a loss\n",
    "    \n",
    "reward_state = RewardState()\n",
    "video_cache = VideoFrameCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7f00dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "apply_stop = False\n",
    "\n",
    "last_i = 0\n",
    "drop_i = 16\n",
    "i = 0\n",
    "\n",
    "for x, obs, reward, terminated, truncated, info in env_iter.iter_all(): \n",
    "    video_cache.cache_append(obs)\n",
    "    model_probs = model(x)\n",
    "    \n",
    "    i_since_r = i - last_i\n",
    "    probs, ptruncated = process_probs(model_probs, i_since_r, timer_i=timer_i, corner_correct=corner_correct)\n",
    "\n",
    "    action = jnp.random.choice(actions, p=probs).item()   \n",
    "    adj_reward = reward_state.step(action, probs, reward, info, prev_lives, i_since_r, timer_i=timer_i) \n",
    "    \n",
    "    ######################################################\n",
    "    if apply_stop:\n",
    "        truncated = truncated or ptruncated\n",
    "    elif i_since_r > 100_000:\n",
    "        warnings.warn('')\n",
    "        truncated = True\n",
    "        \n",
    "    if terminated: # an episode finished\n",
    "        print(f'\\nEpisode {episode_number} of {n_episodes}, Iterations : {i}, Reward : {reward_sum}       \\n\\n', end='\\r')\n",
    "        # Finish The Episode.save\n",
    "        model.episode_backward()\n",
    "        # Finish the Batch\n",
    "        if not env_iter.n_episodes % batch_size:           \n",
    "            model.batch_backward()\n",
    "            if save_path:\n",
    "                model.save(save_path)\n",
    "            gc.collect()\n",
    "    elif truncated: # an episode terminated unexpectedly, shouldn't maintain results\n",
    "        model.episode_losses.clear()\n",
    "      \n",
    "    else:\n",
    "        print(i, end='                          \\r')\n",
    "        \n",
    "    if terminated or truncated:\n",
    "        video_cache.finish(f'episode_vids/episode_{env_iter.n_episodes}.mp4')\n",
    "        env_iter.reset()\n",
    "        reward_state.reset()\n",
    "        \n",
    "        prev_lives = 3 # for new episode adjustment\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111d293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=runs\n",
    "\n",
    "if record_probs:\n",
    "    plot_probs(prob_list)\n",
    "if record_rewards:\n",
    "    plot_rewards(reward_list, window_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a520cf70",
   "metadata": {},
   "source": [
    "[TensorBoard](localhost:6006/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
