{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df726742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install \"gymnasium[atari]\"\n",
    "# !python -m pip install \"gymnasium[accept-rom-license, atari]\"\n",
    "# !pip install shimmy\n",
    "# !pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567e4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "import ale_py\n",
    "import shimmy\n",
    "import joblib\n",
    "import torch\n",
    "import numba\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from video_frame_cache import VideoFrameCache\n",
    "from skimage.measure import block_reduce\n",
    "from IPython.display import clear_output\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable\n",
    "from pathlib import Path\n",
    "from gym import wrappers\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as ln\n",
    "import optax\n",
    "import jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ead628",
   "metadata": {},
   "source": [
    "# Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28496867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_obs(obs):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(obs)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15faaca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(probs, i_since_r, timer_i, c=1, sigma=None, buffer=None):        \n",
    "    # autograd no inplace ops\n",
    "    if buffer is None:\n",
    "        buffer = timer_i // 2\n",
    "    n = len(probs)\n",
    "    if sigma is None:\n",
    "        sigma = 2 / n\n",
    "    noise = torch.normal(0., sigma, (1, n), requires_grad=True) # means, stds shared, size n\n",
    "    noise = noise - noise.mean()\n",
    "    \n",
    "    scale = c * i_since_r / (timer_i - buffer)\n",
    "\n",
    "    probs = probs + noise * scale\n",
    "    pmin = torch.min(probs)\n",
    "    if pmin < 0:\n",
    "        probs = probs - pmin\n",
    "        probs = probs / torch.sum(probs)\n",
    "        \n",
    "    return probs\n",
    "\n",
    "def balance_lr(probs, i_since_r, timer_i, beta=.5, buffer=None):\n",
    "#     2 : 'RIGHT'\n",
    "#     3 : 'LEFT'\n",
    "#     4 : 'RIGHTFIRE'\n",
    "#     5 : 'LEFTFIRE'\n",
    "    if i_since_r < timer_i / 4:\n",
    "        return probs\n",
    "    elif i_since_r < timer_i / 2:\n",
    "        alpha = .5\n",
    "    elif i_since_r < timer_i * 3 / 4:\n",
    "        alpha = .8\n",
    "    else:\n",
    "        alpha = .99\n",
    "\n",
    "    zero_probs = torch.zeros_like(probs, requires_grad=True)\n",
    "    zero_probs[0,2] = (probs[0,3] - probs[0,2])\n",
    "    zero_probs[0,3] = (probs[0,2] - probs[0,3])\n",
    "    \n",
    "    zero_probs[0,4] = (probs[0,5] - probs[0,4])\n",
    "    zero_probs[0,5] = (probs[0,4] - probs[0,5])\n",
    "    zero_probs = zero_probs * alpha * beta / 2\n",
    "    \n",
    "    probs = probs + zero_probs\n",
    "    with torch.no_grad():\n",
    "        assert torch.sum(probs).round(decimals=3) == 1, torch.sum(probs)\n",
    "    return probs\n",
    "\n",
    "def standardize(x):\n",
    "    eps = np.finfo(np.float64).eps.item()\n",
    "    x = (x - x.mean()) / (x.std() + eps)\n",
    "    return x\n",
    "\n",
    "def balance_all(probs, i_since_r, timer_i, beta=2):\n",
    "    probs = probs + 2 * i_since_r / timer_i\n",
    "    probs = softmax = nn.Softmax(dim=-1)(probs)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8147847",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(ln.Module):\n",
    "    num_classes : int = 6\n",
    "    hidden_sizes : tuple = (128, 64)\n",
    "    kernel_init : Callable = ln.initializers.glorot_normal\n",
    "\n",
    "    @ln.compact\n",
    "    def __call__(self, x, return_activations=False):\n",
    "        activations = list()\n",
    "        for hidden_size in self.hidden_sizes:\n",
    "            x = ln.Dense(\n",
    "                hidden_size,\n",
    "                kernel_init=self.kernel_init\n",
    "            )(x)\n",
    "            activations.append(x)\n",
    "            x = jax.nn.swish(x)\n",
    "            activations.append(x)\n",
    "            \n",
    "        x = ln.Dense(\n",
    "            self.num_classes,\n",
    "            kernel_init=self.kernel_init\n",
    "        )(x)\n",
    "        x = jax.nn.sigmoid(x)\n",
    "        activations.append(x)\n",
    "        return x if not return_activations else (x, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef2dacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardState():\n",
    "    def __init__(self, gamma=.99, reward_dict=None):\n",
    "        self.gamma = gamma\n",
    "        if reward_dict is None:\n",
    "            reward_dict = {\n",
    "                'life_penalty' : 15,\n",
    "                'nofire_penalty' : .1,\n",
    "                'comeback_reward' : 10,\n",
    "            }\n",
    "            \n",
    "        self.reward_dict = reward_dict\n",
    "        self.reward_sum = 0\n",
    "        self.adj_reward_sum = 0\n",
    "        self.episode_rewards = list()\n",
    "        self.probss = list()\n",
    "        return\n",
    "    \n",
    "    def episode_reset(self, truncated=False):           \n",
    "        self.reward_sum = 0\n",
    "        self.adj_reward_sum = 0\n",
    "        self.prev_lives = 3\n",
    "        if not truncated:\n",
    "            self.batch_rewards += self.episode_rewards\n",
    "        self.episode_rewards.clear()\n",
    "        return episode_reward\n",
    "    \n",
    "    def batch_reset(self):\n",
    "        self.episode_reset()\n",
    "        self.batch_rewards.clear()\n",
    "        return\n",
    "    \n",
    "    def step(self, reward, *args, **kwargs):\n",
    "        adj_reward = self.modify(reward, *args, **kwargs)\n",
    "        self.reward_sum += reward\n",
    "        self.adj_reward_sum += adj_reward\n",
    "        self.rewards.append(adj_reward)\n",
    "        return adj_reward\n",
    "    \n",
    "    def modify(self, action, reward, info, i_since_r, timer_i=1000):\n",
    "        if info['lives'] < self.prev_lives:\n",
    "            reward += reward_dict['life_penalty']\n",
    "        if reward <= 0 and action in [1,4,5]:\n",
    "            reward += reward_dict['nofire_penalty']\n",
    "        if reward > 0 and i_since_r > timer_i / 2:\n",
    "            reward += reward_dict['comeback_reward']\n",
    "            \n",
    "        self.prev_lives = info['lives']\n",
    "        return reward\n",
    "    \n",
    "    @staticmethod\n",
    "    @numba.jit(nopython=True)\n",
    "    def _discount_rewards(rewards, gamma):\n",
    "        running_add = 0\n",
    "        discounted_rewards = list()\n",
    "        for reward in reversed(rewards):\n",
    "            running_add = running_add * gamma + reward\n",
    "            discounted_rewards.append(running_add)\n",
    "            \n",
    "        discounted_rewards = jnp.asarray(list(reversed(discounted_rewards)))\n",
    "        discounted_rewards = discounted_rewards - discounted_rewards.mean()\n",
    "        discounted_rewards = discounted_rewards / discounted_rewards.std()\n",
    "        return discounted_rewards\n",
    "    \n",
    "    @property\n",
    "    def discount_rewards(self):\n",
    "        discounted_rewards = self._discount_rewards(self.rewards, self.gamma)\n",
    "        self.rewards.clear()\n",
    "        return discounted_rewards\n",
    "    \n",
    "    # uses .dot with discounted rewards as a loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cde6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class JAXReinforcementBase():\n",
    "    model : ln.Module\n",
    "    reward_state : RewardState\n",
    "    optimizer : Callable\n",
    "    obs_shape : tuple\n",
    "    xmin : int=26\n",
    "    xmax : int=196\n",
    "    ymin : int=10\n",
    "    ymax : int=144\n",
    "    downsample : str='horizontal'\n",
    "    timer_i : int=1000\n",
    "    corner_correct : bool=True\n",
    "    seed : int=42\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.episode_rewards = list()\n",
    "        self.episode_probss = list()\n",
    "        self.episode_actions = list()\n",
    "        self.probss = list()\n",
    "        self.actions = list()\n",
    "        \n",
    "        self.key = jax.random.key(self.seed)\n",
    "        key1, key2 = jax.random.split(self.key)\n",
    "        \n",
    "        np_obs = np.zeros(self.obs_shape)\n",
    "#         np_obs[0:10] = 144 # this is just to pass the pre_process assertion\n",
    "        np_x = self.pre_process(np_obs, run_asserts=False)\n",
    "\n",
    "        init_x = jax.random.normal(key1, np_x.shape, dtype=jnp.float32)\n",
    "        self.params = self.model.init(key2, init_x)\n",
    "        \n",
    "        self.opt_state = optimizer.init(self.params)\n",
    "        self.loss_grad_fn = jax.value_and_grad(_self.loss)\n",
    "        return\n",
    "        \n",
    "    def step(self, obs, i_since_r, append_logits=True):\n",
    "        x = self.pre_process(obs)- self.prev_obs\n",
    "        \n",
    "        self.prev_obs = x\n",
    "        x = jnp.asarray(x)\n",
    "        \n",
    "        logits = self.model.apply(self.params, x)\n",
    "        probs, loop_truncated = self.process_probs(logits, i_since_r)\n",
    "        if append_logits:\n",
    "            self.episode_probss.append(logits)\n",
    "        else:\n",
    "            self.episode_probss.append(probs)\n",
    "\n",
    "        key = jax.random.split(self.key)\n",
    "        action = jnp.random.choice(key, actions, p=probs).item()\n",
    "        self.episode_actions.append(action)\n",
    "        \n",
    "        adj_reward = self.reward_state.step(\n",
    "            action, \n",
    "            probs, \n",
    "            reward, \n",
    "            info, \n",
    "            i_since_r, \n",
    "            timer_i=self.timer_i,\n",
    "        ) \n",
    "        return loop_truncated\n",
    "        \n",
    "    @property\n",
    "    def log_probss(self):\n",
    "        log_probss = jnp.log(jnp.concatenate(self.probss, axis=0))\n",
    "        return log_probss\n",
    "        \n",
    "    @property\n",
    "    def one_hot_actions(self):\n",
    "        one_hot_actions = jax.nn.one_hot(self.actions, num_classes=self.model.num_classes)\n",
    "        return one_hot_actions\n",
    "    \n",
    "    def show_layers(self):\n",
    "        jax.tree_util.tree_map(lambda x: x.shape, self.params)\n",
    "        return\n",
    "    \n",
    "    #maybe move concatenates into loss or batch backward?\n",
    "    @staticmethod\n",
    "    def _loss(log_probss, one_hot_actions, discounted_rewards):\n",
    "        loss = -jnp.mean(\n",
    "            jnp.sum(\n",
    "                log_probss * one_hot_actions,\n",
    "                axis=-1,\n",
    "            ) * discounted_rewards\n",
    "        )\n",
    "        print('loss shape', loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    @jax.jit\n",
    "    def _backward(params, opt_state, loss_grad_fn, log_probss, one_hot_actions, discounted_rewards):\n",
    "        loss_val, grads = loss_grad_fn(log_probss, one_hot_actions, discounted_rewards)(params)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return loss_val, params, opt_state\n",
    "    \n",
    "    def batch_backward(self):\n",
    "        assert log_probss.ndim ==2, log_probss.shape\n",
    "        # ignores loss value for the moment\n",
    "        _, self.params, self.opt_state = self._backward(\n",
    "            self.params, \n",
    "            self.opt_state,\n",
    "            self.loss_grad_fn,\n",
    "            self.log_probss,\n",
    "            self.one_hot_actions,\n",
    "            self.reward_state.discounted_rewards,\n",
    "        )\n",
    "        \n",
    "        self.batch_reset()\n",
    "        gc.collect()\n",
    "        return self\n",
    "    \n",
    "    def save(self, bytes_path):\n",
    "        model_bytes = flax.serialization.to_bytes(self.params)\n",
    "        # could chunk this\n",
    "        with open(bytes_path, 'wb') as f:\n",
    "            f.write(model_bytes)\n",
    "        return\n",
    "    \n",
    "    def load(self, bytes_path):\n",
    "        with open(bytes_path, 'rb') as f:\n",
    "            model_bytes = f.read()\n",
    "            \n",
    "        self.params = flax.serialization.from_bytes(self.params, model_bytes)\n",
    "        return\n",
    "    \n",
    "    def episode_reset(self, truncated=False):\n",
    "        episode_reward = self.reward_state.episode_reset(truncated=truncated)\n",
    "        if not truncated:\n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            self.probss += self.episode_probss\n",
    "            self.actions += self.episode_actions\n",
    "            \n",
    "        self.episode_probss.clear()\n",
    "        self.episode_actions.clear()\n",
    "        self.prev_obs = 0\n",
    "        return\n",
    "    \n",
    "    def batch_reset(self):\n",
    "        self.reward_state.batch_reset()\n",
    "        self.probss.clear()\n",
    "        self.actions.clear()\n",
    "        return\n",
    "        \n",
    "    def pre_process(self, obs, run_asserts=True):\n",
    "        if run_asserts:\n",
    "            assert obs.shape == self.obs_shape\n",
    "        obs = obs[self.xmin:self.xmax,self.ymin:self.ymax]\n",
    "        \n",
    "        if self.downsample == 'horizontal':\n",
    "            obs = obs[::2,:]\n",
    "            \n",
    "        obs[obs == 144] = 0 # erase background (background type 1)\n",
    "        obs[obs == 109] = 0 # erase background (background type 2)\n",
    "        obs[obs != 0] = 1 # everything else to 1\n",
    "        print(np.unique(obs))\n",
    "        if run_asserts:\n",
    "            assert len(np.unique(obs)) == 2, np.unique(obs)\n",
    "        \n",
    "        if self.downsample == 'max_pool':\n",
    "            # ideally downsampling would be done before changing values in place, but this way the background is ignored easily\n",
    "            obs = block_reduce(obs, (2, 2), np.amax)\n",
    "        \n",
    "        x = np.expand_dims(obs.ravel().astype(np.float32), axis=0)\n",
    "        return x\n",
    "    \n",
    "    def process_probs(\n",
    "        self,\n",
    "        probs,\n",
    "        i_since_r,\n",
    "    ):\n",
    "        truncated = i_since_r > timer_i\n",
    "\n",
    "        if corner_correct: # heavily biases agent from getting 'stuck' in corner\n",
    "            probs = add_noise(probs, i_since_r, timer_i)\n",
    "            probs = balance_lr(probs, i_since_r, timer_i)\n",
    "        else:\n",
    "            probs = balance_all(probs, i_since_r, timer_i)\n",
    "\n",
    "        if torch.round(torch.sum(probs), decimals=4) != 1:\n",
    "            raise ValueError('Probs do not sum to 1')\n",
    "\n",
    "        return probs, truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddd9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvIter():\n",
    "    def __init__(self, game_name, max_episodes=100, **make_kwargs):\n",
    "        self.env = gym.make(game_name, **make_kwargs)\n",
    "        self.max_episodes = max_episodes\n",
    "        self.n_episodes = -1\n",
    "        self.reset()\n",
    "        self.prev_obs = None\n",
    "        return\n",
    "    \n",
    "    def standard_step(self):\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        if reward > 0:\n",
    "            self.last_i = i\n",
    "            self.i_since_r = 0\n",
    "        else:\n",
    "            self.i_since_r += 1\n",
    "            \n",
    "        self.i += 1\n",
    "        return self.i, self.i_since_r, obs, reward, terminated, truncated, info\n",
    "        \n",
    "    def reset_step(self):\n",
    "        obs, info = self.env.reset()\n",
    "        return 0, 0, obs, 0, False, False, info\n",
    "    \n",
    "    def reset(self, truncated=False):\n",
    "        self.reset_ = True\n",
    "        if not truncated:\n",
    "            self.n_episodes += 1\n",
    "        self.i = 0\n",
    "        self.last_i = 0\n",
    "        return\n",
    "    \n",
    "    def iter_all(self):\n",
    "        while self.n_episodes <= self.max_episodes:\n",
    "            if self.reset_:\n",
    "                self.reset_ = False\n",
    "                yield self.reset_step()\n",
    "            else:\n",
    "                yield self.standard_step()\n",
    "        env.close()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc401574",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dict = {\n",
    "    0 : 'NOOP',\n",
    "    1 : 'FIRE',\n",
    "    2 : 'RIGHT',\n",
    "    3 : 'LEFT',\n",
    "    4 : 'RIGHTFIRE',\n",
    "    5 : 'LEFTFIRE'\n",
    "}\n",
    "actions = sorted(action_dict)\n",
    "\n",
    "batch_size = 64\n",
    "n_batches = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d5052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(\n",
    "    num_classes=len(actions),\n",
    ")\n",
    "\n",
    "video_cache = VideoFrameCache()\n",
    "\n",
    "# EnvIter should change so that preprocessing is only in base\n",
    "env_iter = EnvIter(\n",
    "    'ALE/DemonAttack-v5', \n",
    "    base.pre_process,\n",
    "    n_episodes=10,\n",
    "    obs_type='grayscale', \n",
    "    render_mode=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7f00dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_batches(\n",
    "    env_iter, \n",
    "    model,\n",
    "    obs_shape=(210, 160),\n",
    "    video_cache=None, \n",
    "    max_i=5000, \n",
    "    batch_size=64, \n",
    "    n_batches=16,\n",
    "    lr=.01,\n",
    "    **base_kwargs,\n",
    "):\n",
    "    reward_state = RewardState()\n",
    "    base = JAXReinforcementBase(\n",
    "        model,\n",
    "        reward_state,\n",
    "        optax.adam(learning_rate=lr),\n",
    "        obs_shape,\n",
    "        **base_kwargs,\n",
    "    )\n",
    "    base.model_init()\n",
    "\n",
    "    for i, i_since_r, obs, reward, terminated, truncated, info in env_iter.iter_all(): \n",
    "        if video_cache:\n",
    "            video_cache.cache_append(obs)\n",
    "\n",
    "        loop_truncated = base.step(x, i_since_r)\n",
    "        truncated = truncated or loop_truncated or i >= max_i\n",
    "        ######################################################\n",
    "\n",
    "        if terminated: # an episode finished intentionally\n",
    "            print(f'\\nEpisode {episode_number} of {n_episodes}, Iterations : {i}, Reward : {reward_sum}       \\n\\n', end='\\r')\n",
    "            base.episode_reset(truncated=False)\n",
    "            if not env_iter.n_episodes % batch_size:           \n",
    "                base.batch_backward()\n",
    "        elif truncated: # an episode terminated unexpectedly, shouldn't maintain results\n",
    "            base.episode_reset(truncated=True)\n",
    "        else:\n",
    "            print(i, end='                          \\r')\n",
    "\n",
    "        if terminated or truncated:\n",
    "            if video_cache:\n",
    "                video_cache.finish(f'episode_vids/episode_{env_iter.n_episodes}.mp4')\n",
    "            env_iter.reset(truncated=truncated)\n",
    "            \n",
    "    avg_reward = np.mean(base.episode_rewards)\n",
    "    return base.params, avg_reward\n",
    "\n",
    "# set up building model from parameters\n",
    "params, avg_reward = run_batches(\n",
    "    env_iter, \n",
    "    model,\n",
    "    obs_shape=(210, 160),\n",
    "    video_cache=video_cache, \n",
    "    max_i=5000, \n",
    "    batch_size=batch_size, \n",
    "    n_batches=n_batches,\n",
    "    lr=.01,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
