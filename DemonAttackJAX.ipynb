{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df726742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install \"gymnasium[atari]\"\n",
    "# !python -m pip install \"gymnasium[accept-rom-license, atari]\"\n",
    "# !pip install shimmy\n",
    "# !pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "567e4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "import ale_py\n",
    "import shimmy\n",
    "import joblib\n",
    "import torch\n",
    "import numba\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from video_frame_cache import VideoFrameCache\n",
    "from skimage.measure import block_reduce\n",
    "from IPython.display import clear_output\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable\n",
    "from pathlib import Path\n",
    "from gym import wrappers\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as ln\n",
    "import optax\n",
    "import jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ead628",
   "metadata": {},
   "source": [
    "# Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28496867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_obs(obs):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(obs)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15faaca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(probs, i_since_r, timer_i, c=1, sigma=None, buffer=None):        \n",
    "    # autograd no inplace ops\n",
    "    if buffer is None:\n",
    "        buffer = timer_i // 2\n",
    "    n = len(probs)\n",
    "    if sigma is None:\n",
    "        sigma = 2 / n\n",
    "    noise = torch.normal(0., sigma, (1, n), requires_grad=True) # means, stds shared, size n\n",
    "    noise = noise - noise.mean()\n",
    "    \n",
    "    scale = c * i_since_r / (timer_i - buffer)\n",
    "\n",
    "    probs = probs + noise * scale\n",
    "    pmin = torch.min(probs)\n",
    "    if pmin < 0:\n",
    "        probs = probs - pmin\n",
    "        probs = probs / torch.sum(probs)\n",
    "        \n",
    "    return probs\n",
    "\n",
    "def balance_lr(probs, i_since_r, timer_i, beta=.5, buffer=None):\n",
    "#     2 : 'RIGHT'\n",
    "#     3 : 'LEFT'\n",
    "#     4 : 'RIGHTFIRE'\n",
    "#     5 : 'LEFTFIRE'\n",
    "    if i_since_r < timer_i / 4:\n",
    "        return probs\n",
    "    elif i_since_r < timer_i / 2:\n",
    "        alpha = .5\n",
    "    elif i_since_r < timer_i * 3 / 4:\n",
    "        alpha = .8\n",
    "    else:\n",
    "        alpha = .99\n",
    "\n",
    "    zero_probs = torch.zeros_like(probs, requires_grad=True)\n",
    "    zero_probs[0,2] = (probs[0,3] - probs[0,2])\n",
    "    zero_probs[0,3] = (probs[0,2] - probs[0,3])\n",
    "    \n",
    "    zero_probs[0,4] = (probs[0,5] - probs[0,4])\n",
    "    zero_probs[0,5] = (probs[0,4] - probs[0,5])\n",
    "    zero_probs = zero_probs * alpha * beta / 2\n",
    "    \n",
    "    probs = probs + zero_probs\n",
    "    with torch.no_grad():\n",
    "        assert torch.sum(probs).round(decimals=3) == 1, torch.sum(probs)\n",
    "    return probs\n",
    "\n",
    "def standardize(x):\n",
    "    eps = np.finfo(np.float64).eps.item()\n",
    "    x = (x - x.mean()) / (x.std() + eps)\n",
    "    return x\n",
    "\n",
    "def balance_all(probs, i_since_r, timer_i, beta=2):\n",
    "    probs = probs + 2 * i_since_r / timer_i\n",
    "    probs = softmax = nn.Softmax(dim=-1)(probs)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e7dd4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(ln.Module):\n",
    "    num_classes : int = 6\n",
    "    hidden_sizes : tuple = (128, 64)\n",
    "    kernel_init : Callable = ln.initializers.glorot_normal\n",
    "\n",
    "    @ln.compact\n",
    "    def __call__(self, x, return_activations=False):\n",
    "        activations = list()\n",
    "        for hidden_size in self.hidden_sizes:\n",
    "            x = ln.Dense(\n",
    "                hidden_size,\n",
    "                kernel_init=self.kernel_init\n",
    "            )(x)\n",
    "            activations.append(x)\n",
    "            x = jax.nn.swish(x)\n",
    "            activations.append(x)\n",
    "            \n",
    "        x = ln.Dense(\n",
    "            self.num_classes,\n",
    "            kernel_init=self.kernel_init\n",
    "        )(x)\n",
    "        x = jax.nn.sigmoid(x)\n",
    "        activations.append(x)\n",
    "        return x if not return_activations else (x, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc1d7a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardState():\n",
    "    def __init__(self, gamma=.99, reward_dict=None):\n",
    "        self.gamma = gamma\n",
    "        if reward_dict is None:\n",
    "            reward_dict = {\n",
    "                'life_penalty' : 15,\n",
    "                'nofire_penalty' : .1,\n",
    "                'comeback_reward' : 10,\n",
    "            }\n",
    "            \n",
    "        self.reward_dict = reward_dict\n",
    "        self.reward_sum = 0\n",
    "        self.adj_reward_sum = 0\n",
    "        self.episode_rewards = list()\n",
    "        self.probss = list()\n",
    "        return\n",
    "    \n",
    "    def episode_reset(self, truncated=False):           \n",
    "        self.reward_sum = 0\n",
    "        self.adj_reward_sum = 0\n",
    "        self.prev_lives = 3\n",
    "        if not truncated:\n",
    "            self.batch_rewards += self.episode_rewards\n",
    "        self.episode_rewards.clear()\n",
    "        return episode_reward\n",
    "    \n",
    "    def batch_reset(self):\n",
    "        self.episode_reset()\n",
    "        self.batch_rewards.clear()\n",
    "        return\n",
    "    \n",
    "    def step(self, reward, *args, **kwargs):\n",
    "        adj_reward = self.modify(reward, *args, **kwargs)\n",
    "        self.reward_sum += reward\n",
    "        self.adj_reward_sum += adj_reward\n",
    "        self.rewards.append(adj_reward)\n",
    "        return adj_reward\n",
    "    \n",
    "    def modify(self, action, reward, info, i_since_r, timer_i=1000):\n",
    "        if info['lives'] < self.prev_lives:\n",
    "            reward += reward_dict['life_penalty']\n",
    "        if reward <= 0 and action in [1,4,5]:\n",
    "            reward += reward_dict['nofire_penalty']\n",
    "        if reward > 0 and i_since_r > timer_i / 2:\n",
    "            reward += reward_dict['comeback_reward']\n",
    "            \n",
    "        self.prev_lives = info['lives']\n",
    "        return reward\n",
    "    \n",
    "    @staticmethod\n",
    "    @numba.jit(nopython=True)\n",
    "    def _discount_rewards(rewards, gamma):\n",
    "        running_add = 0\n",
    "        discounted_rewards = list()\n",
    "        for reward in reversed(rewards):\n",
    "            running_add = running_add * gamma + reward\n",
    "            discounted_rewards.append(running_add)\n",
    "            \n",
    "        discounted_rewards = jnp.asarray(list(reversed(discounted_rewards)))\n",
    "        discounted_rewards = discounted_rewards - discounted_rewards.mean()\n",
    "        discounted_rewards = discounted_rewards / discounted_rewards.std()\n",
    "        return discounted_rewards\n",
    "    \n",
    "    def discount_rewards(self):\n",
    "        discounted_rewards = self._discount_rewards(self.rewards, self.gamma)\n",
    "        self.rewards.clear()\n",
    "        return discounted_rewards\n",
    "    \n",
    "    # uses .dot with discounted rewards as a loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61cde6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class JAXReinforcementBase():\n",
    "    model : ln.Module\n",
    "    reward_state : RewardState\n",
    "    optimizer : Callable\n",
    "    obs_shape : tuple\n",
    "    xmin : int=26\n",
    "    xmax : int=196\n",
    "    ymin : int=10\n",
    "    ymax : int=144\n",
    "    downsample : str='horizontal'\n",
    "    timer_i : int=1000\n",
    "    corner_correct : bool=True\n",
    "    seed : int=42\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.episode_rewards = list()\n",
    "        self.episode_probss = list()\n",
    "        self.probss = list()\n",
    "        \n",
    "        self.key = jax.random.key(self.seed)\n",
    "        key1, key2 = jax.random.split(self.key)\n",
    "        \n",
    "        np_obs = np.zeros(self.obs_shape)\n",
    "#         np_obs[0:10] = 144 # this is just to pass the pre_process assertion\n",
    "        np_x = self.pre_process(np_obs, run_asserts=False)\n",
    "\n",
    "        init_x = jax.random.normal(key1, np_x.shape, dtype=jnp.float32)\n",
    "        self.params = self.model.init(key2, init_x)\n",
    "        \n",
    "        self.opt_state = optimizer.init(self.params)\n",
    "        self.loss_grad_fn = jax.value_and_grad(self.loss)\n",
    "        return\n",
    "        \n",
    "    def step(self, obs, i_since_r):\n",
    "        x = self.pre_process(obs)- self.prev_obs\n",
    "        \n",
    "        self.prev_obs = x\n",
    "        x = jnp.asarray(x)\n",
    "        \n",
    "        model_probs = self.model.apply(self.params, x)\n",
    "        probs = self.process_probs(model_probs, i_since_r)\n",
    "        self.episode_probss.append(probs)\n",
    "\n",
    "        key = jax.random.split(self.key)\n",
    "        action = jnp.random.choice(key, actions, p=probs).item()   \n",
    "        \n",
    "        adj_reward = self.reward_state.step(\n",
    "            action, \n",
    "            probs, \n",
    "            reward, \n",
    "            info, \n",
    "            i_since_r, \n",
    "            timer_i=self.timer_i,\n",
    "        ) \n",
    "        return action\n",
    "        \n",
    "    def log_probss(self):\n",
    "        log_probss = jnp.log(jnp.concatenate(self.probss, axis=0))\n",
    "        return log_probss\n",
    "        \n",
    "    def show_layers(self):\n",
    "        jax.tree_util.tree_map(lambda x: x.shape, self.params)\n",
    "        return\n",
    "    \n",
    "    #maybe move concatenates into loss or batch backward?\n",
    "    @staticmethod\n",
    "    def loss(log_probss, discounted_rewards):\n",
    "        loss = jnp.dot(log_probss, discounted_rewards)\n",
    "        print('loss shape', loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    @jax.jit\n",
    "    def _backward(params, opt_state, loss_grad_fn, log_probss, discounted_rewards):\n",
    "        loss_val, grads = loss_grad_fn(log_probss, discounted_rewards)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state\n",
    "    \n",
    "    def batch_backward(self):\n",
    "        log_probss = self.log_probss()\n",
    "        assert log_probss.ndim ==2, log_probss.shape\n",
    "        discounted_rewards = self.reward_state.discounted_rewards()\n",
    "        self.params, self.opt_state = self._backward(\n",
    "            self.params, \n",
    "            self.opt_state,\n",
    "            self.loss_grad_fn,\n",
    "            log_probss,\n",
    "            discounted_rewards,\n",
    "        )\n",
    "        \n",
    "        self.batch_reset()\n",
    "        gc.collect()\n",
    "        return self\n",
    "    \n",
    "    def save(self, bytes_path):\n",
    "        model_bytes = flax.serialization.to_bytes(self.params)\n",
    "        # could chunk this\n",
    "        with open(bytes_path, 'wb') as f:\n",
    "            f.write(model_bytes)\n",
    "        return\n",
    "    \n",
    "    def load(self, bytes_path):\n",
    "        with open(bytes_path, 'rb') as f:\n",
    "            model_bytes = f.read()\n",
    "            \n",
    "        self.params = flax.serialization.from_bytes(self.params, model_bytes)\n",
    "        return\n",
    "    \n",
    "    def episode_reset(self, truncated=False):\n",
    "        episode_reward = self.reward_state.episode_reset(truncated=truncated)\n",
    "        if not truncated:\n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            self.probss += self.episode_probss\n",
    "            \n",
    "        self.episode_probss.clear()\n",
    "        self.prev_obs = 0\n",
    "        return\n",
    "    \n",
    "    def batch_reset(self):\n",
    "        self.reward_state.batch_reset()\n",
    "        self.probss.clear()\n",
    "        return\n",
    "        \n",
    "    def pre_process(self, obs, run_asserts=True):\n",
    "        if run_asserts:\n",
    "            assert obs.shape == self.obs_shape\n",
    "        obs = obs[self.xmin:self.xmax,self.ymin:self.ymax]\n",
    "        \n",
    "        if self.downsample == 'horizontal':\n",
    "            obs = obs[::2,:]\n",
    "            \n",
    "        obs[obs == 144] = 0 # erase background (background type 1)\n",
    "        obs[obs == 109] = 0 # erase background (background type 2)\n",
    "        obs[obs != 0] = 1 # everything else to 1\n",
    "        print(np.unique(obs))\n",
    "        if run_asserts:\n",
    "            assert len(np.unique(obs)) == 2, np.unique(obs)\n",
    "        \n",
    "        if self.downsample == 'max_pool':\n",
    "            # ideally downsampling would be done before changing values in place, but this way the background is ignored easily\n",
    "            obs = block_reduce(obs, (2, 2), np.amax)\n",
    "        \n",
    "        x = np.expand_dims(obs.ravel().astype(np.float32), axis=0)\n",
    "        return x\n",
    "    \n",
    "    def process_probs(\n",
    "        self,\n",
    "        probs,\n",
    "        i_since_r,\n",
    "    ):\n",
    "        truncated = i_since_r > timer_i\n",
    "\n",
    "        if corner_correct: # heavily biases agent from getting 'stuck' in corner\n",
    "            probs = add_noise(probs, i_since_r, timer_i)\n",
    "            probs = balance_lr(probs, i_since_r, timer_i)\n",
    "        else:\n",
    "            probs = balance_all(probs, i_since_r, timer_i)\n",
    "\n",
    "        if torch.round(torch.sum(probs), decimals=4) != 1:\n",
    "            raise ValueError('Probs do not sum to 1')\n",
    "\n",
    "        return probs, truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bddd9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvIter():\n",
    "    def __init__(self, game_name, max_episodes=100, **make_kwargs):\n",
    "        self.env = gym.make(game_name, **make_kwargs)\n",
    "        self.max_episodes = max_episodes\n",
    "        self.n_episodes = -1\n",
    "        self.reset()\n",
    "        self.prev_obs = None\n",
    "        return\n",
    "    \n",
    "    def standard_step(self):\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        if reward > 0:\n",
    "            self.last_i = i\n",
    "            self.i_since_r = 0\n",
    "        else:\n",
    "            self.i_since_r += 1\n",
    "            \n",
    "        self.i += 1\n",
    "        return self.i, self.i_since_r, obs, reward, terminated, truncated, info\n",
    "        \n",
    "    def reset_step(self):\n",
    "        obs, info = self.env.reset()\n",
    "        return 0, 0, obs, 0, False, False, info\n",
    "    \n",
    "    def reset(self, truncated=False):\n",
    "        self.reset_ = True\n",
    "        if not truncated:\n",
    "            self.n_episodes += 1\n",
    "        self.i = 0\n",
    "        self.last_i = 0\n",
    "        return\n",
    "    \n",
    "    def iter_all(self):\n",
    "        while self.n_episodes <= self.max_episodes:\n",
    "            if self.reset_:\n",
    "                self.reset_ = False\n",
    "                yield self.reset_step()\n",
    "            else:\n",
    "                yield self.standard_step()\n",
    "        env.close()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc401574",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dict = {\n",
    "    0 : 'NOOP',\n",
    "    1 : 'FIRE',\n",
    "    2 : 'RIGHT',\n",
    "    3 : 'LEFT',\n",
    "    4 : 'RIGHTFIRE',\n",
    "    5 : 'LEFTFIRE'\n",
    "}\n",
    "actions = sorted(action_dict)\n",
    "\n",
    "batch_size = 64\n",
    "n_batches = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06d5052d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Value '<function variance_scaling.<locals>.init at 0x000002645ECD3C70>' with dtype object is not a valid JAX array type. Only arrays of numeric types are supported by JAX.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\_src\\dtypes.py:611\u001b[0m, in \u001b[0;36mdtype\u001b[1;34m(x, canonicalize)\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 611\u001b[0m   dt \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot interpret '<function variance_scaling.<locals>.init at 0x000002645ECD3C70>' as a data type",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:2053\u001b[0m, in \u001b[0;36marray\u001b[1;34m(object, dtype, copy, order, ndmin)\u001b[0m\n\u001b[0;32m   2052\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2053\u001b[0m   dtype \u001b[38;5;241m=\u001b[39m \u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lattice_result_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mleaves\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m leaves \u001b[38;5;28;01melse\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39mfloat_\n\u001b[0;32m   2054\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   2055\u001b[0m   \u001b[38;5;66;03m# This happens if, e.g. one of the entries is a memoryview object.\u001b[39;00m\n\u001b[0;32m   2056\u001b[0m   \u001b[38;5;66;03m# This is rare, so we only handle it if the normal path fails.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\_src\\dtypes.py:621\u001b[0m, in \u001b[0;36m_lattice_result_type\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lattice_result_type\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[DType, \u001b[38;5;28mbool\u001b[39m]:\n\u001b[1;32m--> 621\u001b[0m   dtypes, weak_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_dtype_and_weaktype\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dtypes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\_src\\dtypes.py:621\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lattice_result_type\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[DType, \u001b[38;5;28mbool\u001b[39m]:\n\u001b[1;32m--> 621\u001b[0m   dtypes, weak_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[43m_dtype_and_weaktype\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args))\n\u001b[0;32m    622\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dtypes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\_src\\dtypes.py:429\u001b[0m, in \u001b[0;36m_dtype_and_weaktype\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;124;03m\"\"\"Return a (dtype, weak_type) tuple for the given input.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28many\u001b[39m(value \u001b[38;5;129;01mis\u001b[39;00m typ \u001b[38;5;28;01mfor\u001b[39;00m typ \u001b[38;5;129;01min\u001b[39;00m _weak_types) \u001b[38;5;129;01mor\u001b[39;00m is_weakly_typed(value)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\_src\\dtypes.py:613\u001b[0m, in \u001b[0;36mdtype\u001b[1;34m(x, canonicalize)\u001b[0m\n\u001b[0;32m    612\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 613\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot determine dtype of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _jax_dtype_set \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issubdtype(dt, extended):\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot determine dtype of <function variance_scaling.<locals>.init at 0x000002645ECD3C70>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearModel(\n\u001b[0;32m      2\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(actions),\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      5\u001b[0m reward_state \u001b[38;5;241m=\u001b[39m RewardState()\n\u001b[1;32m----> 7\u001b[0m base \u001b[38;5;241m=\u001b[39m \u001b[43mJAXReinforcementBase\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreward_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobs_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorner_correct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorner_correct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimer_i\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimer_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m base\u001b[38;5;241m.\u001b[39mmodel_init()\n\u001b[0;32m     17\u001b[0m video_cache \u001b[38;5;241m=\u001b[39m VideoFrameCache()\n",
      "File \u001b[1;32m<string>:15\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, model, reward_state, optimizer, obs_shape, xmin, xmax, ymin, ymax, downsample, timer_i, corner_correct, seed)\u001b[0m\n",
      "Cell \u001b[1;32mIn[8], line 29\u001b[0m, in \u001b[0;36mJAXReinforcementBase.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     26\u001b[0m np_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_process(np_obs, run_asserts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     28\u001b[0m init_x \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(key1, np_x\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mjnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_state \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39minit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_grad_fn \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvalue_and_grad(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss)\n",
      "    \u001b[1;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m, in \u001b[0;36mLinearModel.__call__\u001b[1;34m(self, x, return_activations)\u001b[0m\n\u001b[0;32m      8\u001b[0m activations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hidden_size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_sizes:\n\u001b[1;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mln\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_init\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     activations\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[0;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mswish(x)\n",
      "    \u001b[1;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flax\\linen\\linear.py:247\u001b[0m, in \u001b[0;36mDense.__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    246\u001b[0m   bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m inputs, kernel, bias \u001b[38;5;241m=\u001b[39m \u001b[43mpromote_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdot_general_cls \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    250\u001b[0m   dot_general \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdot_general_cls()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flax\\linen\\dtypes.py:95\u001b[0m, in \u001b[0;36mpromote_dtype\u001b[1;34m(dtype, inexact, *args)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpromote_dtype\u001b[39m(\u001b[38;5;241m*\u001b[39margs, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, inexact\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Array]:\n\u001b[0;32m     72\u001b[0m   \u001b[38;5;124;03m\"\"\" \"Promotes input arguments to a specified or inferred dtype.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m  All args are cast to the same dtype. See ``canonicalize_dtype`` for how\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;124;03m    The arguments cast to arrays of the same dtype.\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m   dtype \u001b[38;5;241m=\u001b[39m \u001b[43mcanonicalize_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minexact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minexact\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [jnp\u001b[38;5;241m.\u001b[39masarray(x, dtype) \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flax\\linen\\dtypes.py:62\u001b[0m, in \u001b[0;36mcanonicalize_dtype\u001b[1;34m(dtype, inexact, *args)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m\"\"\"Canonicalize an optional dtype to the definitive dtype.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03mIf the ``dtype`` is None this function will infer the dtype. If it is not\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m  The dtype that *args should be cast to.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m   args_filtered \u001b[38;5;241m=\u001b[39m [jnp\u001b[38;5;241m.\u001b[39masarray(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m     63\u001b[0m   dtype \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mresult_type(\u001b[38;5;241m*\u001b[39margs_filtered)\n\u001b[0;32m     64\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m inexact \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39missubdtype(dtype, jnp\u001b[38;5;241m.\u001b[39minexact):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flax\\linen\\dtypes.py:62\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m\"\"\"Canonicalize an optional dtype to the definitive dtype.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03mIf the ``dtype`` is None this function will infer the dtype. If it is not\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m  The dtype that *args should be cast to.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m   args_filtered \u001b[38;5;241m=\u001b[39m [\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m     63\u001b[0m   dtype \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mresult_type(\u001b[38;5;241m*\u001b[39margs_filtered)\n\u001b[0;32m     64\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m inexact \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39missubdtype(dtype, jnp\u001b[38;5;241m.\u001b[39minexact):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:2110\u001b[0m, in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2109\u001b[0m   dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(dtype, allow_extended_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m-> 2110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:2058\u001b[0m, in \u001b[0;36marray\u001b[1;34m(object, dtype, copy, order, ndmin)\u001b[0m\n\u001b[0;32m   2054\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   2055\u001b[0m     \u001b[38;5;66;03m# This happens if, e.g. one of the entries is a memoryview object.\u001b[39;00m\n\u001b[0;32m   2056\u001b[0m     \u001b[38;5;66;03m# This is rare, so we only handle it if the normal path fails.\u001b[39;00m\n\u001b[0;32m   2057\u001b[0m     leaves \u001b[38;5;241m=\u001b[39m [_convert_to_array_if_dtype_fails(leaf) \u001b[38;5;28;01mfor\u001b[39;00m leaf \u001b[38;5;129;01min\u001b[39;00m leaves]\n\u001b[1;32m-> 2058\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lattice_result_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mleaves\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   2060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m weak_type:\n\u001b[0;32m   2061\u001b[0m   dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(dtype, allow_extended_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\_src\\dtypes.py:615\u001b[0m, in \u001b[0;36mdtype\u001b[1;34m(x, canonicalize)\u001b[0m\n\u001b[0;32m    613\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot determine dtype of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _jax_dtype_set \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issubdtype(dt, extended):\n\u001b[1;32m--> 615\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid JAX array \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    616\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype. Only arrays of numeric types are supported by JAX.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    617\u001b[0m \u001b[38;5;66;03m# TODO(jakevdp): fix return type annotation and remove this ignore.\u001b[39;00m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m canonicalize_dtype(dt, allow_extended_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m canonicalize \u001b[38;5;28;01melse\u001b[39;00m dt\n",
      "\u001b[1;31mTypeError\u001b[0m: Value '<function variance_scaling.<locals>.init at 0x000002645ECD3C70>' with dtype object is not a valid JAX array type. Only arrays of numeric types are supported by JAX."
     ]
    }
   ],
   "source": [
    "model = LinearModel(\n",
    "    num_classes=len(actions),\n",
    ")\n",
    "\n",
    "video_cache = VideoFrameCache()\n",
    "\n",
    "# EnvIter should change so that preprocessing is only in base\n",
    "env_iter = EnvIter(\n",
    "    'ALE/DemonAttack-v5', \n",
    "    base.pre_process,\n",
    "    n_episodes=10,\n",
    "    obs_type='grayscale', \n",
    "    render_mode=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7f00dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_batches(\n",
    "    env_iter, \n",
    "    model,\n",
    "    obs_shape=(210, 160),\n",
    "    video_cache=None, \n",
    "    max_i=5000, \n",
    "    batch_size=64, \n",
    "    n_batches=16,\n",
    "    lr=.01,\n",
    "    **base_kwargs,\n",
    "):\n",
    "    reward_state = RewardState()\n",
    "    base = JAXReinforcementBase(\n",
    "        model,\n",
    "        reward_state,\n",
    "        optax.adam(learning_rate=lr),\n",
    "        obs_shape,\n",
    "        **base_kwargs,\n",
    "    )\n",
    "    base.model_init()\n",
    "\n",
    "    for i, i_since_r, obs, reward, terminated, truncated, info in env_iter.iter_all(): \n",
    "        if video_cache:\n",
    "            video_cache.cache_append(obs)\n",
    "\n",
    "        model_probs, probs, action, loop_truncated = base.step(x, i_since_r)\n",
    "        truncated = truncated or loop_truncated or i >= max_i\n",
    "        ######################################################\n",
    "\n",
    "        if terminated: # an episode finished intentionally\n",
    "            print(f'\\nEpisode {episode_number} of {n_episodes}, Iterations : {i}, Reward : {reward_sum}       \\n\\n', end='\\r')\n",
    "            base.episode_reset(truncated=False)\n",
    "            if not env_iter.n_episodes % batch_size:           \n",
    "                base.batch_backward()\n",
    "        elif truncated: # an episode terminated unexpectedly, shouldn't maintain results\n",
    "            base.episode_reset(truncated=True)\n",
    "        else:\n",
    "            print(i, end='                          \\r')\n",
    "\n",
    "        if terminated or truncated:\n",
    "            if video_cache:\n",
    "                video_cache.finish(f'episode_vids/episode_{env_iter.n_episodes}.mp4')\n",
    "            env_iter.reset(truncated=truncated)\n",
    "            \n",
    "    avg_reward = np.mean(base.episode_rewards)\n",
    "    return base.params, avg_reward\n",
    "\n",
    "# set up building model from parameters\n",
    "params, avg_reward = run_batches(\n",
    "    env_iter, \n",
    "    model,\n",
    "    obs_shape=(210, 160),\n",
    "    video_cache=video_cache, \n",
    "    max_i=5000, \n",
    "    batch_size=batch_size, \n",
    "    n_batches=n_batches,\n",
    "    lr=.01,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
