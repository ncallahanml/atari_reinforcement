# Atari Deep Reinforcement

## Problem

Simple version developed for a Reinforcment Learning semester project, which is an expansion from a NumPy implementation of a Q learning network for Pong. The intent is to create a simple deep reinforcement learning network that improves on Atari environments generated by the [`gymnasium`]() environment, a continuation of the [`gym`]() package from OpenAI. The Atari game DemonAttack is used specifically, for no particular purpose except that it requires additional considerations not required of Pong reinforcement. The network should be able to adapt to any Atari game supplied by `gymnasium` after the output classes, preprocessing routine, and custom rewards are adjusted.

## Implementation Library
### NumPy

This version was intended simply to show learning progress, there were only necessary corrections made for stopping the network from getting caught in a loop, and simple visualization. The network is simple, as initially increasing the size of the network while holding other parameters constant decreased the actual improvement rate. Using only a few linear layers and RMSProp, the network learned to improve the average episode score from ~100 to ~400 over the course of 1200 episodes, although this was done in a few learning leaps.

### Torch

The NumPy version offered a fast and relatively simple network that demonstrated significant improvements over random actions, but without the benefit of autograd and integration of alternative optimizers, the code is challenging to directly expand on and is highly coupled, making it hard to comprehend. By reworking the network in PyTorch the breadth of optimizers, prebuilt layers and autograd already available in most neural networking libraries is now at the disposal of the network developer, allowing faster and cleaner improvements that have the option to run on GPUs. This version focuses of clearer plots and additional reinforcement techniques demonstrating gradual but consistent improvement.

## Additions

A few common problems with reinforcement networks arise when learning DemonAttack with a naive action network, as such some typical adjustments are added into the training notebooks. These have been added with justification, but not empirically tested individually to elicit their true contribution to different aspects of the reinforcement process.

### Probabilistic Actions

A naive model would predict an exact action for every input state, but this is extremely unstable, as networks will tend to overexploit specific actions, and their long term performance will suffer as a result of the greedy choices made at each action step. Therefore, the network outputs softmaxed logits instead of a single output.

```python
def forward(self, x):
    x = self.input(x)
    ...
    x = self.output(x)
    probs = self.softmax(x)
    return probs
```

Later on, a categorical distribution is used to actually select an action, rather than simply taking the maximum softmaxed value, a standard process for deep reinforcement networks.

```python
probs = model(x)
action_dist = torch.distributions.Categorical(probs)
action = action_dist.sample()
```
 
### Modified Rewards

The rewards for Demon Attack revolve around scoring points for blasting demons, which come in waves of three, getting stronger every two waves until they reach maximum potentcy on the 11th wave. As the demons are upgraded, they also have a higher reward when being killed, between 10 points for the initial demons to 140 points for the diving demons introduced at level 11. Below is the full reward structure supplied by gymnasium.

| Wave | Demons | Split Demons | Diving Demons |
|:----:|:------:|:------------:|:-------------:|
| 1,2  |   10   |   --         |     --        |
| 3,4  |   15   |   --         |     --        |
| 5,6  |   20   |   40         |     80        |
| 7,8  |   25   |   50         |     100       |
| 9,10 |   30   |   60         |     120       |
| 11,..|   35   |   70         |     140       |

There are no negative rewards in this system, the game only ends when the user is out of lives, which are represented in the game state but not immediately visible through the reward system. To discourage actions that lead to deaths, the following negative reward is included.

| Action | Reward | Goal |
|:------:|:------:|:----:|
| Death  | -15    | Discourage actions causing a death |

Some more optional rewards are available to encourage different agent behaviors, their impact has yet to be thoroughly assessed.

| Action | Reward | Goal |
|:------:|:------:|:----:|
| Firing without Reward | -1 | Improve accuracy |
| Successful Hit after No Reward Period | 30 | Provide extra incentive for recovering from a period where inaction/unsucessful actions were being taken continuously |
| NOOP without Reward | -.1 | Discourage inactivity, which can lead to no reward periods that slow learning significantly |

### Encouraging Exploration

Even with probabilistic action selection, the network can quickly get stuck in suboptimal patterns. A variety of parameters exist to adjust this, such as modifying the size of batches or the learning rate, but a common tactic is to also inject noise into the system.

This is especially necessary when actors are susceptible to looping, where they enter a pattern that leads to extreme exploitation or get stuck in a pattern that will not result in rewards. The latter is very easy to experience in Demon Attack, due to the movement pattern of the demons the actor can be moving with little deviation on one side of the scene while the demon only moves with little deviation on the opposite side, leading to a stalled game state. In order to improve long term learning and decrease likelihood of stalled games, adding noise to the predictions of the model is necessary, particularly in the earlier stages where less of the state space has been explored.

#### Monte Carlo Dropout

Adding noise to the predictions provides a consistent way to explore the state space, however the addition of gaussian noise to predictions forces new actions without respect to the actual choices the model would make. To compromise between pure noise and simple probabilistic selection from softmax outputs, dropout can be applied _while making predictions_, rather than just while training. This provides variability in results, reducing possibility of exploitation as the model gradually forgets what it has learned. 

Ideally, information that has been learned across many nodes in the model, which tends to be more robust, is the last sort of information lost when making a prediction. Therefore, predictions made with increased dropout are not necessarily more random, just more robust. The current strategy applies an increasingly large dropout probability when there has been a period of no positive rewards being gained.

#### Left-Right Correction

The stalled game states occur when the agent has imbalanced probabilities of moving left vs right. In order to correct for this, if an agent has gone a long time without achieving any reward, the difference between the left side and right side probabilities can be reduced until the agent has gained a reward. Coupling this with an increased reward for breaking a stalled state theoretically helps the network learn the importance of generic balance between left and right movements when there isn't an immediate reward to be gained by one or the other.

#### Reversion to Random

The simplest strategy to ensure the model explores different actions is to add significant noise to the probabilistic outputs, reducing the effect of what the model has already learned in order to measure reward associated with all actions. This is especially valuable when the model hasn't built up a robust strategy, or when it is explicitly failing in some capacity. Continuing the theme of addressing situations where the model has not achieved any rewards, adding noise to the probabilistic outputs (and rescaling them) is effective, in proportion to how much the model has trained and/or how long it has been since a reward was achieved.
