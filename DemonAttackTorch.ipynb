{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df726742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install \"gymnasium[atari]\"\n",
    "# !python -m pip install \"gymnasium[accept-rom-license, atari]\"\n",
    "# !pip install shimmy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567e4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "import ale_py\n",
    "import shimmy\n",
    "import joblib\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b7597b",
   "metadata": {},
   "source": [
    "| **Value** | **Meaning** |\n",
    "|:---------:|:-----------:|\n",
    "| 0 | NOOP |\n",
    "| 1 | FIRE |\n",
    "| 2 | RIGHT |\n",
    "| 3 | LEFT |\n",
    "| 4 | RIGHTFIRE |\n",
    "| 5 | LEFTFIRE |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ffbd68",
   "metadata": {},
   "source": [
    "# Run Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7513bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config flags - video output and res\n",
    "resume = True # resume training from previous checkpoint (from save.p  file)?\n",
    "render = False # render video output?\n",
    "print_ = False # print each observation\n",
    "show = False\n",
    "no_grad = False\n",
    "corner_correct = True\n",
    "plot_action_every = 64\n",
    "\n",
    "timer_i = 1000 # number of iterations without reward before noise is intentionally greater than signal\n",
    "\n",
    "record_actions = True # will be handled as true if plot_action_every \n",
    "record_probs = True\n",
    "record_rewards = True\n",
    "save_path = 'model.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ead628",
   "metadata": {},
   "source": [
    "# Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc47b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "OBS_SHAPE = (210, 160)\n",
    "XMIN = 26\n",
    "XMAX = 196\n",
    "YMIN = 14\n",
    "YMAX = 144\n",
    "SHAPE = (XMAX - XMIN, YMAX - YMIN)\n",
    "DOWNSAMPLE = False\n",
    "DIM = np.prod(SHAPE) // 4 if DOWNSAMPLE else np.prod(SHAPE)\n",
    "\n",
    "action_dict = {\n",
    "    0 : 'NOOP',\n",
    "    1 : 'FIRE',\n",
    "    2 : 'RIGHT',\n",
    "    3 : 'LEFT',\n",
    "    4 : 'RIGHTFIRE',\n",
    "    5 : 'LEFTFIRE'\n",
    "}\n",
    "ACTIONS = [0,1,2,3,4,5] # modify to limit available actions\n",
    "N_CLASSES = len(ACTIONS)\n",
    "\n",
    "print('Input Shape:', SHAPE)\n",
    "print('Input Dimensionality:', DIM)\n",
    "\n",
    "def preprocess(obs, downsample=True, xmin=26, xmax=196, ymin=10, ymax=144):\n",
    "    assert obs.shape == (210, 160)\n",
    "    I = obs[xmin:xmax,ymin:ymax] # crop - remove 35px from start & 25px from end of image in x, to reduce redundant parts of image (i.e. after ball passes paddle)\n",
    "    if downsample:\n",
    "        I = I[::2,:]\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else to 1\n",
    "#     return I.astype(np.float32).ravel() # ravel flattens an array and collapses it into a column vector\n",
    "    obs = torch.from_numpy(I.ravel()).double().unsqueeze(dim=0)\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28496867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_obs(obs):\n",
    "    \"\"\" \n",
    "    Simple display of image observation \n",
    "    \n",
    "    Args:\n",
    "    `obs` : np.ndarray\n",
    "    - Observation from the environment\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(obs)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def plot_action_counts(action_list):\n",
    "    fig, ax = plt.subplots(1,1,figsize=(6,6))\n",
    "\n",
    "    transform = lambda x : action_dict[int(x)]\n",
    "    action_series = pd.Series(action_list, name='Count').dropna()\n",
    "    data = action_series.value_counts().sort_index()\n",
    "    data /= data.sum() # normalize to probabilities\n",
    "    data.index = list(map(transform, data.index))\n",
    "    data = data.to_frame().transpose()\n",
    "    sns.barplot(data=data, palette='Spectral', ax=ax)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15faaca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(probs, i, i_since_r, timer_i, buffer=None, print_=False):\n",
    "    # autograd no inplace ops\n",
    "    if buffer is None:\n",
    "        buffer = timer_i // 2\n",
    "    n = len(probs)\n",
    "    sigma = 2 / n\n",
    "    noise = torch.normal(0., sigma, (n,)) # means, stds shared, size n\n",
    "    noise = noise - noise.mean()\n",
    "    \n",
    "    scale = i_since_r / (timer_i - buffer)\n",
    "    noise = noise * scale\n",
    "#     assert not round(noise.mean(), 3), noise\n",
    "    new_probs = probs + noise\n",
    "    pmin = torch.min(new_probs)\n",
    "    if pmin < 0:\n",
    "        new_probs = new_probs - pmin\n",
    "        new_probs = new_probs / np.sum(new_probs)\n",
    "    return new_probs\n",
    "\n",
    "def balance_lr(probs, i_since_r, timer_i, buffer=None):\n",
    "#    ACTIONS = [NOOP,1,2,3,4,5]\n",
    "    raise NotImplementedError()\n",
    "    if i_since_r < timer_i // 4:\n",
    "        pass\n",
    "    elif i_since_r < timer_i // 2:\n",
    "        equal_n = (probs[2] + probs[3]) / 2\n",
    "        equal_y = (probs[4] + probs[5]) / 2\n",
    "        probs[2] = equal_n\n",
    "        probs[3] = equal_n\n",
    "        probs[4] = equal_y\n",
    "        probs[5] = equal_y\n",
    "    elif i_since_r < 3 * timer_i // 4:\n",
    "        probs[2], probs[3] = probs[3], probs[2]\n",
    "        probs[4], probs[5] = probs[5], probs[4]\n",
    "    return probs\n",
    "    \n",
    "def modify_reward(action, reward, info, prev_lives):\n",
    "    if info['lives'] < prev_lives:\n",
    "        reward -= 15\n",
    "    if reward <= 0 and action in [1,4,5]:\n",
    "        reward -= 1\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a186eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_probs(probs, i, last_i, total_max=1_000_000, timer_i=1000, corner_correct=True):\n",
    "    initial_shape = probs.shape\n",
    "    i_since_r = i - last_i\n",
    "    if i_since_r > timer_i or i > total_max:\n",
    "        terminated = True\n",
    "        truncated = False\n",
    "        print('Timer causing reset               ')\n",
    "    else:\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        \n",
    "    if corner_correct: # heavily biases agent from getting 'stuck' in corner\n",
    "        probs = add_noise(probs, i, i_since_r, timer_i)\n",
    "#         probs = balance_lr(probs, i_since_r, timer_i)\n",
    "\n",
    "    if torch.round(torch.sum(probs), decimals=4) != 1:\n",
    "        warnings.warn(str(probs) + ' | ' + str(torch.sum(probs)) + ' != 1')\n",
    "        probs = probs / torch.sum(probs)\n",
    "        \n",
    "    assert probs.shape == initial_shape\n",
    "    return probs, i, terminated, truncated\n",
    "\n",
    "# def discount_rewards(rewards, gamma=.99):\n",
    "#     \"\"\" \n",
    "#     Take 1D array of rewards and compute discounted version\n",
    "#     Most recent action has the greatest weight \n",
    "\n",
    "#     Args:\n",
    "#     `rewards` : np.ndarray\n",
    "#     - Observed rewards over time\n",
    "#     - ndim : 1\n",
    "#     \"\"\"\n",
    "#     discounted_rewards = torch.empty(len(rewards)).double()\n",
    "#     running_add = 0\n",
    "#     for i, reward in zip(range(len(rewards) - 1, -1, -1), rewards):\n",
    "#         running_add = running_add * gamma + reward  \n",
    "#         discounted_rewards[i] = running_add\n",
    "#     return discounted_rewards\n",
    "\n",
    "def standardize(returns):\n",
    "    eps = np.finfo(np.float64).eps.item()\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb49d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerReinforcement(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, gamma=.99):\n",
    "        super(TwoLayerReinforcement, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.log_probss = list()\n",
    "        self.rewards = list()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.optimizer = torch.optim.RMSprop(self.parameters(), lr=0.001)\n",
    "        return\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 2\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer2(x)\n",
    "        probs = self.softmax(x)\n",
    "#         probs = torch.nn.functional.softmax(x, dim=0)\n",
    "        return probs\n",
    "\n",
    "    def discount_rewards(self):\n",
    "        running_add = 0\n",
    "        discounted_rewards = list()\n",
    "        for reward in self.rewards:\n",
    "            running_add = running_add * self.gamma + reward\n",
    "            discounted_rewards.append(running_add)\n",
    "        discounted_rewards = torch.tensor(list(reversed(discounted_rewards)))\n",
    "        return discounted_rewards\n",
    "\n",
    "    def reward_loss(self):\n",
    "        assert len(self.rewards)\n",
    "\n",
    "        discounted_rewards = self.discount_rewards()\n",
    "        discounted_rewards = standardize(discounted_rewards)\n",
    "        episode_losss = [(-log_prob * reward).unsqueeze(dim=0) for log_prob, r in zip(self.log_probss, discounted_rewards)]\n",
    "\n",
    "        loss = torch.cat(episode_losss).double().sum()\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        assert len(self.log_probss)\n",
    "        self.optimizer.zero_grad() # make sure this shouldn't go in reward_loss\n",
    "        loss = self.reward_loss()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        model.log_probss.clear()\n",
    "        model.rewards.clear()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e56cb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerReinforcement(DIM, 64, 6).double()\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7f00dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_episodes = 64 * 7 + 1\n",
    "batch_size = 64\n",
    "\n",
    "prev_x = None # used in computing the difference frame\n",
    "reward_sum = 0\n",
    "adj_reward_sum = 0\n",
    "last_i = 0\n",
    "env = gym.make(\n",
    "    'ALE/DemonAttack-v5', # alternate games can be chosen here \n",
    "    obs_type='grayscale', # saves RGB preprocessing reduction\n",
    "    render_mode='human' if render else None, # rendering shows popup but limits training speed\n",
    ")\n",
    "\n",
    "if record_actions:\n",
    "    action_list = list()\n",
    "if record_rewards:\n",
    "    reward_list = list()\n",
    "if record_probs:\n",
    "    prob_list = list()\n",
    "\n",
    "episode_number = 0\n",
    "prev_lives = 0\n",
    "i = 0\n",
    "\n",
    "obs, info = env.reset()\n",
    "while episode_number <= n_episodes:\n",
    "    curr_x = preprocess(obs, downsample=DOWNSAMPLE, xmin=XMIN, xmax=XMAX, ymin=YMIN, ymax=YMAX)\n",
    "    x = curr_x - prev_x if prev_x is not None else torch.zeros((1, DIM)).double() # only monitor change between frames\n",
    "    prev_x = curr_x\n",
    "    assert x.dim() == 2, x.dim()\n",
    "    model_probs = model(x) # autograd performed here\n",
    "    probs, last_i, terminated, truncated = process_probs(model_probs, i, last_i, timer_i=timer_i, corner_correct=corner_correct)\n",
    "    \n",
    "    if record_probs:\n",
    "        prob_list.append(probs)\n",
    "        \n",
    "    m = torch.distributions.Categorical(probs)\n",
    "    action = m.sample() \n",
    "    \n",
    "    log_probs = m.log_prob(action)\n",
    "    model.log_probss.append(log_probs)\n",
    "    action = action.item()\n",
    "    \n",
    "    if record_actions or plot_action_every:\n",
    "        action_list.append(action)\n",
    "    ######################################################################################\n",
    "    # fold this into model\n",
    "    # fix log probs here\n",
    "    # https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py\n",
    "    # log_prob = prob.clamp(min=1e-6).log()\n",
    "    # entropy = - (probs * probs.clamp(min=1e-6).log()).sum()\n",
    "    # https://discuss.pytorch.org/t/policy-gradient-using-loss-as-reward/13877\n",
    "    ######################################################################################\n",
    "    \n",
    "    prev_lives = info['lives'] # lives not available through general step return\n",
    "    obs, reward, terminated, truncated, info = env.step(action) # step returns all other relevant information \n",
    "    if reward > 0: # reset the iterations since last reward if reward is accrued\n",
    "        last_i = i\n",
    "\n",
    "    reward_sum += reward # total round reward incremented\n",
    "#     adj_reward = modify_reward(action, reward, info, prev_lives) # adjusted reward may better lead agent toward short term optimums\n",
    "    adj_reward = reward\n",
    "    adj_reward_sum += adj_reward\n",
    "\n",
    "    model.rewards.append(adj_reward)\n",
    "    ######################################################\n",
    "\n",
    "    \n",
    "        \n",
    "    if terminated: # an episode finished\n",
    "        episode_number += 1\n",
    "\n",
    "        # Finish The Episode\n",
    "        if not no_grad and not episode_number % batch_size:\n",
    "            model.backward()\n",
    "            \n",
    "        if plot_action_every and not episode_number % plot_action_every:\n",
    "            plot_action_counts(action_list)\n",
    "            action_list.clear()\n",
    "        \n",
    "        if record_rewards:\n",
    "            reward_list.append(reward_sum)\n",
    "\n",
    "        reward_sum = 0 # reset all totals\n",
    "        adj_reward_sum = 0\n",
    "        \n",
    "        obs, info = env.reset() # reset env\n",
    "        prev_x = None\n",
    "        i = 0\n",
    "    elif truncated: # an episode terminated unexpectedly, shouldn't maintain results\n",
    "        model.log_probss.clear()\n",
    "        model.rewards.clear()\n",
    "        \n",
    "        reward_sum = 0\n",
    "        adj_reward_sum = 0\n",
    "        \n",
    "        obs, info = env.reset()\n",
    "        prev_x = None\n",
    "        i = 0\n",
    "           \n",
    "    if not i % 100:\n",
    "        print(f'Episode {episode_number} of {n_episodes} episodes                ', end='\\r')\n",
    "#         torch.save(model, save_path)\n",
    "    i += 1\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299817c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probs(prob_list, step=1):\n",
    "    probs_arr = torch.vstack(prob_list).detach().numpy()\n",
    "    display(probs_arr.shape)\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(16,12), dpi=200, sharex=True, sharey=True)\n",
    "    fig.suptitle('Single Episode Action Probabilities')\n",
    "    colors = sns.color_palette('Spectral', 7)\n",
    "    colors = colors[:3] + colors[4:]\n",
    "    for i, (ax, color) in enumerate(zip(axs.flatten(), colors)):\n",
    "        sns.lineplot(data=probs_arr[::step,i], color=color, label=action_dict[i], alpha=.7, dashes=False, ax=ax)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "if record_probs:\n",
    "    plot_probs(prob_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111d293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, window_size) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[window_size:] = ret[window_size:] - ret[:-window_size]\n",
    "    return ret[window_size - 1:] / window_size\n",
    "\n",
    "def plot_rewards(reward_list, window_size=10):\n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.title('Rewards Over Time')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.xlabel('Episode Number')\n",
    "    x = np.arange(0, len(reward_list), 1)\n",
    "    assert len(x) == len(reward_list)\n",
    "    plt.plot(x, reward_list, color='black', linestyle='dashed', label='Reward Per Episode')\n",
    "    plt.plot(x[window_size-1:], moving_average(reward_list, window_size), color='red', label='Reward Moving Average')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "if record_rewards:\n",
    "    plot_rewards(reward_list, window_size=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
