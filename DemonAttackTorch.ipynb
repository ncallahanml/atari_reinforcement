{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df726742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install \"gymnasium[atari]\"\n",
    "# !python -m pip install \"gymnasium[accept-rom-license, atari]\"\n",
    "# !pip install shimmy\n",
    "# !pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "567e4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "import ale_py\n",
    "import shimmy\n",
    "import joblib\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from gym import wrappers\n",
    "from skimage.measure import block_reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b7597b",
   "metadata": {},
   "source": [
    "| **Value** | **Meaning** |\n",
    "|:---------:|:-----------:|\n",
    "| 0 | NOOP |\n",
    "| 1 | FIRE |\n",
    "| 2 | RIGHT |\n",
    "| 3 | LEFT |\n",
    "| 4 | RIGHTFIRE |\n",
    "| 5 | LEFTFIRE |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ffbd68",
   "metadata": {},
   "source": [
    "# Run Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e7513bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = False # resume training from previous checkpoint\n",
    "render = False # render video output?\n",
    "print_ = False # print each observation\n",
    "show = False\n",
    "no_grad = False\n",
    "corner_correct = True\n",
    "plot_action_every = 64\n",
    "\n",
    "timer_i = 1000 # number of iterations without reward before noise is intentionally greater than signal\n",
    "max_i = 2000\n",
    "\n",
    "record_actions = True # will be handled as true if plot_action_every \n",
    "record_probs = True\n",
    "record_rewards = True\n",
    "save_path = 'model.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ead628",
   "metadata": {},
   "source": [
    "# Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc47b8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: (170, 130)\n",
      "Input Dimensionality: 5525\n"
     ]
    }
   ],
   "source": [
    "OBS_SHAPE = (210, 160)\n",
    "XMIN = 26\n",
    "XMAX = 196\n",
    "YMIN = 14\n",
    "YMAX = 144\n",
    "SHAPE = (XMAX - XMIN, YMAX - YMIN)\n",
    "downsample = 'max_pool'\n",
    "dim = np.prod(SHAPE)\n",
    "if downsample == 'horizontal':\n",
    "    dim //= 2\n",
    "elif downsample == 'max_pool':\n",
    "    dim //= 4\n",
    "\n",
    "action_dict = {\n",
    "    0 : 'NOOP',\n",
    "    1 : 'FIRE',\n",
    "    2 : 'RIGHT',\n",
    "    3 : 'LEFT',\n",
    "    4 : 'RIGHTFIRE',\n",
    "    5 : 'LEFTFIRE'\n",
    "}\n",
    "ACTIONS = [0,1,2,3,4,5] # modify to limit available actions\n",
    "N_CLASSES = len(ACTIONS)\n",
    "\n",
    "print('Input Shape:', SHAPE)\n",
    "print('Input Dimensionality:', dim)\n",
    "\n",
    "def preprocess(obs, downsample='max_pool', xmin=26, xmax=196, ymin=10, ymax=144):\n",
    "    assert obs.shape == (210, 160)\n",
    "    I = obs[xmin:xmax,ymin:ymax] # crop - remove 35px from start & 25px from end of image in x, to reduce redundant parts of image (i.e. after ball passes paddle)\n",
    "    if downsample == 'horizontal':\n",
    "        I = I[::2,:]\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else to 1\n",
    "    if downsample == 'max_pool':\n",
    "        # ideally downsampling would be done before changing values in place, but this way the background is ignored easily\n",
    "        I = block_reduce(I, (2, 2), np.amax)\n",
    "#     return I.astype(np.float32).ravel() # ravel flattens an array and collapses it into a column vector\n",
    "    obs = torch.from_numpy(I.ravel()).double().unsqueeze(dim=0)\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28496867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_obs(obs):\n",
    "    \"\"\" \n",
    "    Simple display of image observation \n",
    "    \n",
    "    Args:\n",
    "    `obs` : np.ndarray\n",
    "    - Observation from the environment\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(obs)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def plot_action_counts(action_list):\n",
    "    fig, ax = plt.subplots(1,1,figsize=(6,6))\n",
    "\n",
    "    transform = lambda x : action_dict[int(x)]\n",
    "    action_series = pd.Series(action_list, name='Count').dropna()\n",
    "    data = action_series.value_counts().sort_index()\n",
    "    data /= data.sum() # normalize to probabilities\n",
    "    data.index = list(map(transform, data.index))\n",
    "    data = data.to_frame().transpose()\n",
    "    sns.barplot(data=data, palette='Spectral', ax=ax)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15faaca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(probs, i, i_since_r, timer_i, buffer=None, print_=False):\n",
    "        \n",
    "    # autograd no inplace ops\n",
    "    if buffer is None:\n",
    "        buffer = timer_i // 2\n",
    "    n = len(probs)\n",
    "    sigma = 2 / n\n",
    "    noise = torch.normal(0., sigma, (1, n)) # means, stds shared, size n\n",
    "    noise = noise - noise.mean()\n",
    "    \n",
    "    scale = i_since_r / (timer_i - buffer)\n",
    "    noise = noise * scale\n",
    "#     assert not round(noise.mean(), 3), noise\n",
    "    probs = probs + noise\n",
    "    pmin = torch.min(probs)\n",
    "    if pmin < 0:\n",
    "        probs = probs - pmin\n",
    "        probs = probs / torch.sum(probs)\n",
    "        \n",
    "#     assert False, probs.shape\n",
    "    return probs\n",
    "\n",
    "def balance_lr(probs, i_since_r, timer_i, buffer=None):\n",
    "#     2 : 'RIGHT',\n",
    "#     3 : 'LEFT',\n",
    "#     4 : 'RIGHTFIRE',\n",
    "#     5 : 'LEFTFIRE'\n",
    "    if i_since_r < timer_i // 4:\n",
    "        return probs\n",
    "    elif i_since_r < timer_i // 2:\n",
    "        alpha = .5\n",
    "    elif i_since_r < 3 * timer_i // 4:\n",
    "        alpha = .8\n",
    "        \n",
    "    zero_probs = torch.zeros_like(probs)\n",
    "    zero_probs[2] = probs[3] - probs[2] / 2\n",
    "    zero_probs[3] = probs[2] - probs[3] / 2\n",
    "    \n",
    "    zero_probs[4] = probs[5] - probs[4] / 2\n",
    "    zero_probs[5] = probs[4] - probs[5] / 2\n",
    "    zero_probs = zero_probs * alpha / 2\n",
    "    \n",
    "    probs = probs + zero_probs\n",
    "    with torch.no_grad():\n",
    "        assert round(torch.sum(probs), 3) == 1, torch.sum(probs)\n",
    "    return probs\n",
    "    \n",
    "def modify_reward(action, reward, info, prev_lives):\n",
    "    if info['lives'] < prev_lives:\n",
    "        reward -= 15\n",
    "    if reward <= 0 and action in [1,4,5]:\n",
    "        reward -= 1\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "829d65ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_probs(probs, i, last_i, max_i=5_000, timer_i=1000, corner_correct=True):\n",
    "    initial_shape = probs.shape\n",
    "    i_since_r = i - last_i\n",
    "    if i_since_r > timer_i or i_since_r > max_i:\n",
    "        terminated = True\n",
    "        truncated = False\n",
    "        print('\\n\\nTimer causing reset               \\n\\n')\n",
    "    else:\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        \n",
    "    if corner_correct: # heavily biases agent from getting 'stuck' in corner\n",
    "        probs = add_noise(probs, i, i_since_r, timer_i)\n",
    "        probs = balance_lr(probs, i_since_r, timer_i)\n",
    "\n",
    "    if torch.round(torch.sum(probs), decimals=4) != 1:\n",
    "        warnings.warn(str(probs) + ' | ' + str(torch.sum(probs)) + ' != 1')\n",
    "        probs = probs / torch.sum(probs)\n",
    "        \n",
    "    assert probs.shape == initial_shape\n",
    "    return probs, i, terminated, truncated\n",
    "\n",
    "def standardize(returns):\n",
    "    eps = np.finfo(np.float64).eps.item()\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb49d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerReinforcement(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, gamma=.99):\n",
    "        super(TwoLayerReinforcement, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.log_probss = list()\n",
    "        self.rewards = list()\n",
    "#         self.episode_rewards = list()\n",
    "        self.episode_losses = list()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.optimizer = torch.optim.RMSprop(self.parameters(), lr=0.1)\n",
    "        return\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 2\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer2(x)\n",
    "        probs = self.softmax(x)\n",
    "#         probs = torch.nn.functional.softmax(x, dim=0)\n",
    "        return probs\n",
    "\n",
    "    def discount_rewards(self):\n",
    "        running_add = 0\n",
    "        discounted_rewards = list()\n",
    "        for reward in self.rewards:\n",
    "            running_add = running_add * self.gamma + reward\n",
    "            discounted_rewards.append(running_add)\n",
    "        discounted_rewards = torch.tensor(list(reversed(discounted_rewards)))\n",
    "        return discounted_rewards\n",
    "\n",
    "    def reward_loss(self):\n",
    "        assert len(self.rewards)\n",
    "\n",
    "        discounted_rewards = self.discount_rewards()\n",
    "        discounted_rewards = standardize(discounted_rewards)\n",
    "        log_probs = -torch.cat(self.log_probss)\n",
    "        \n",
    "        loss = torch.sum(torch.mul(log_probs, discounted_rewards))\n",
    "        return loss\n",
    "    \n",
    "    def episode_backward(self):\n",
    "        assert len(self.log_probss)\n",
    "        episode_loss = self.reward_loss()\n",
    "        print(episode_loss)\n",
    "        self.episode_losses.append(episode_loss.unsqueeze(0))\n",
    "        \n",
    "        model.log_probss.clear()\n",
    "        model.rewards.clear()\n",
    "        return model\n",
    "    \n",
    "    def batch_backward(self):\n",
    "        assert len(self.episode_losses)\n",
    "        self.optimizer.zero_grad() # make sure this shouldn't go in reward_loss\n",
    "        loss = torch.sum(torch.cat(self.episode_losses))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.episode_losses.clear()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc401574",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerReinforcement(dim, 64, 6).double() \n",
    "if resume:\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc7f00dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574                          \n",
      "Episode 0 of 193 episodes finishing                \n",
      "\n",
      "tensor(-3.7063, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "995                          \n",
      "Episode 1 of 193 episodes finishing                \n",
      "\n",
      "tensor(4.1947, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "1366                          \n",
      "Episode 2 of 193 episodes finishing                \n",
      "\n",
      "tensor(0.4229, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "1210                          \n",
      "Episode 3 of 193 episodes finishing                \n",
      "\n",
      "tensor(-0.0180, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "269                          \n",
      "Episode 4 of 193 episodes finishing                \n",
      "\n",
      "tensor(0.0178, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "336                          \n",
      "Episode 5 of 193 episodes finishing                \n",
      "\n",
      "tensor(0.3076, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "2814                          \n",
      "Episode 6 of 193 episodes finishing                \n",
      "\n",
      "tensor(-2.8403, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "1008                          \n",
      "Episode 7 of 193 episodes finishing                \n",
      "\n",
      "tensor(-2.4757, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "317                          \n",
      "Episode 8 of 193 episodes finishing                \n",
      "\n",
      "tensor(-3.3135, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "813                          \n",
      "Episode 9 of 193 episodes finishing                \n",
      "\n",
      "tensor(-0.7559, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "1265                          \n",
      "Episode 10 of 193 episodes finishing                \n",
      "\n",
      "tensor(-0.4128, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "1024                          \n",
      "Episode 11 of 193 episodes finishing                \n",
      "\n",
      "tensor(0.0501, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "1388                          \n",
      "Episode 12 of 193 episodes finishing                \n",
      "\n",
      "tensor(-0.1969, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "2612                          \n",
      "Episode 13 of 193 episodes finishing                \n",
      "\n",
      "tensor(-3.2631, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "2346                          \n",
      "Episode 14 of 193 episodes finishing                \n",
      "\n",
      "tensor(-4.1508, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "563                          \n",
      "Episode 15 of 193 episodes finishing                \n",
      "\n",
      "tensor(1.3516, dtype=torch.float64, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sum(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 78\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Finish the Batch\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_grad \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m episode_number \u001b[38;5;241m%\u001b[39m batch_size:\n\u001b[1;32m---> 78\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plot_action_every \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m episode_number \u001b[38;5;241m%\u001b[39m plot_action_every:\n\u001b[0;32m     81\u001b[0m     plot_action_counts(action_list)\n",
      "Cell \u001b[1;32mIn[11], line 60\u001b[0m, in \u001b[0;36mTwoLayerReinforcement.batch_backward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_losses)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# make sure this shouldn't go in reward_loss\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepisode_losses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;31mTypeError\u001b[0m: sum(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "n_episodes = 64 * 3 + 1\n",
    "batch_size = 16\n",
    "\n",
    "prev_x = None # used in computing the difference frame\n",
    "reward_sum = 0\n",
    "adj_reward_sum = 0\n",
    "last_i = 0\n",
    "env = gym.make(\n",
    "    'ALE/DemonAttack-v5', # alternate games can be chosen here \n",
    "    obs_type='grayscale', # saves RGB preprocessing reduction\n",
    "    render_mode='human' if render else None, # rendering shows popup but limits training speed\n",
    ")\n",
    "\n",
    "if record_actions:\n",
    "    action_list = list()\n",
    "if record_rewards:\n",
    "    reward_list = list()\n",
    "if record_probs:\n",
    "    prob_list = list()\n",
    "\n",
    "episode_number = 0\n",
    "prev_lives = 0\n",
    "i = 0\n",
    "\n",
    "obs, info = env.reset()\n",
    "while episode_number <= n_episodes:\n",
    "    curr_x = preprocess(obs, downsample=downsample, xmin=XMIN, xmax=XMAX, ymin=YMIN, ymax=YMAX)\n",
    "    x = curr_x - prev_x if prev_x is not None else torch.zeros((1, dim)).double() # only monitor change between frames\n",
    "    prev_x = curr_x\n",
    "    assert x.dim() == 2, x.dim()\n",
    "    model_probs = model(x) # autograd performed here\n",
    "    probs, last_i, terminated, truncated = process_probs(model_probs, i, last_i, max_i=max_i, timer_i=timer_i, corner_correct=corner_correct)\n",
    "    \n",
    "    if record_probs:\n",
    "        prob_list.append(probs)\n",
    "        \n",
    "    m = torch.distributions.Categorical(probs)\n",
    "    action = m.sample() \n",
    "    \n",
    "    log_probs = m.log_prob(action)\n",
    "    model.log_probss.append(log_probs)\n",
    "    action = action.item()\n",
    "    \n",
    "    if record_actions or plot_action_every:\n",
    "        action_list.append(action)\n",
    "    ######################################################################################\n",
    "    # fold this into model\n",
    "    # fix log probs here\n",
    "    # https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py\n",
    "    # log_prob = prob.clamp(min=1e-6).log()\n",
    "    # entropy = - (probs * probs.clamp(min=1e-6).log()).sum()\n",
    "    # https://discuss.pytorch.org/t/policy-gradient-using-loss-as-reward/13877\n",
    "    ######################################################################################\n",
    "    \n",
    "    prev_lives = info['lives'] # lives not available through general step return\n",
    "    obs, reward, terminated, truncated, info = env.step(action) # step returns all other relevant information \n",
    "    if reward > 0: # reset the iterations since last reward if reward is accrued\n",
    "        last_i = i\n",
    "\n",
    "    reward_sum += reward # total round reward incremented\n",
    "    adj_reward = modify_reward(action, reward, info, prev_lives) # adjusted reward may better lead agent toward short term optimums\n",
    "#     adj_reward = reward\n",
    "    adj_reward_sum += adj_reward\n",
    "\n",
    "    model.rewards.append(adj_reward)\n",
    "    ######################################################\n",
    "\n",
    "    if terminated: # an episode finished\n",
    "        print(f'\\nEpisode {episode_number} of {n_episodes} episodes finishing                \\n\\n', end='\\r')\n",
    "        \n",
    "        episode_number += 1\n",
    "\n",
    "        # Finish The Episode\n",
    "        model.episode_backward()\n",
    "        \n",
    "        # Finish the Batch\n",
    "        if not no_grad and not episode_number % batch_size:\n",
    "            model.batch_backward()\n",
    "            \n",
    "        if plot_action_every and not episode_number % plot_action_every:\n",
    "            plot_action_counts(action_list)\n",
    "            action_list.clear()\n",
    "        \n",
    "        if record_rewards:\n",
    "            reward_list.append(reward_sum)\n",
    "\n",
    "        reward_sum = 0 # reset all totals\n",
    "        adj_reward_sum = 0\n",
    "        \n",
    "        obs, info = env.reset() # reset env\n",
    "        prev_x = None\n",
    "        i = 0\n",
    "    elif truncated: # an episode terminated unexpectedly, shouldn't maintain results\n",
    "        model.log_probss.clear()\n",
    "        model.rewards.clear()\n",
    "        \n",
    "        reward_sum = 0\n",
    "        adj_reward_sum = 0\n",
    "        \n",
    "        obs, info = env.reset()\n",
    "        prev_x = None\n",
    "        i = 0\n",
    "    else:\n",
    "        print(i, end='                          \\r')\n",
    "           \n",
    "    \n",
    "#         torch.save(model, save_path)\n",
    "    i += 1\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299817c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probs(prob_list, step=1):\n",
    "    probs_arr = torch.vstack(prob_list).detach().numpy()\n",
    "    display(probs_arr.shape)\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(16,12), dpi=200, sharex=True, sharey=True)\n",
    "    fig.suptitle('Single Episode Action Probabilities')\n",
    "    colors = sns.color_palette('Spectral', 7)\n",
    "    colors = colors[:3] + colors[4:]\n",
    "    for i, (ax, color) in enumerate(zip(axs.flatten(), colors)):\n",
    "        sns.lineplot(data=probs_arr[::step,i], color=color, label=action_dict[i], alpha=.7, dashes=False, ax=ax)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "if record_probs:\n",
    "    plot_probs(prob_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111d293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, window_size) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[window_size:] = ret[window_size:] - ret[:-window_size]\n",
    "    return ret[window_size - 1:] / window_size\n",
    "\n",
    "def plot_rewards(reward_list, window_size=10):\n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.title('Rewards Over Time')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.xlabel('Episode Number')\n",
    "    x = np.arange(0, len(reward_list), 1)\n",
    "    assert len(x) == len(reward_list)\n",
    "    plt.plot(x, reward_list, color='black', linestyle='dashed', label='Reward Per Episode')\n",
    "    plt.plot(x[window_size-1:], moving_average(reward_list, window_size), color='red', label='Reward Moving Average')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "if record_rewards:\n",
    "    plot_rewards(reward_list, window_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95857b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
