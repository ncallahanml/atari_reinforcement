{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df726742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install \"gymnasium[atari]\"\n",
    "# !python -m pip install \"gymnasium[accept-rom-license, atari]\"\n",
    "# !pip install shimmy\n",
    "# !pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "567e4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "import ale_py\n",
    "import shimmy\n",
    "import joblib\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from gym import wrappers\n",
    "from skimage.measure import block_reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b7597b",
   "metadata": {},
   "source": [
    "| **Value** | **Meaning** |\n",
    "|:---------:|:-----------:|\n",
    "| 0 | NOOP |\n",
    "| 1 | FIRE |\n",
    "| 2 | RIGHT |\n",
    "| 3 | LEFT |\n",
    "| 4 | RIGHTFIRE |\n",
    "| 5 | LEFTFIRE |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ffbd68",
   "metadata": {},
   "source": [
    "# Run Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e7513bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = True # resume training from previous checkpoint\n",
    "render = False # render video output?\n",
    "show = False\n",
    "no_grad = False\n",
    "corner_correct = False\n",
    "plot_action_every = 0 # 64\n",
    "\n",
    "timer_i = 2000 # number of iterations without reward before noise is intentionally greater than signal\n",
    "\n",
    "record_actions = False # will be handled as true if plot_action_every > 0\n",
    "record_probs = False\n",
    "record_rewards = True\n",
    "save_path = 'model.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ead628",
   "metadata": {},
   "source": [
    "# Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "461e26d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probs(prob_list, step=1, dpi=200, subplot=True):\n",
    "    probs_arr = torch.vstack(prob_list).detach().numpy()\n",
    "    display(probs_arr.shape)\n",
    "    \n",
    "    if subplot:\n",
    "        fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(16,12), dpi=dpi, sharex=True, sharey=True)\n",
    "    else:\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16,12), dpi=dpi)\n",
    "        axs = [ax] * 6\n",
    "    \n",
    "    fig.suptitle('Single Episode Action Probabilities')\n",
    "    colors = sns.color_palette('Spectral', 7)\n",
    "    colors = colors[:3] + colors[4:]\n",
    "    for i, (ax, color) in enumerate(zip(axs.flatten(), colors)):\n",
    "        sns.lineplot(data=probs_arr[::step,i], color=color, label=action_dict[i], alpha=.7, dashes=False, ax=ax)\n",
    "        ax.set_ylim([0,1])\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def moving_average(a, window_size) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[window_size:] = ret[window_size:] - ret[:-window_size]\n",
    "    return ret[window_size - 1:] / window_size\n",
    "\n",
    "def plot_rewards(reward_list, window_size=10):\n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.title('Rewards Over Time')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.xlabel('Episode Number')\n",
    "    x = np.arange(0, len(reward_list), 1)\n",
    "    assert len(x) == len(reward_list)\n",
    "    plt.scatter(x, reward_list, color='black', label='Reward Per Episode')\n",
    "    plt.plot(x[window_size-1:], moving_average(reward_list, window_size), color='red', label='Reward Moving Average')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28496867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_obs(obs):\n",
    "    \"\"\" \n",
    "    Simple display of image observation \n",
    "    \n",
    "    Args:\n",
    "    `obs` : np.ndarray\n",
    "    - Observation from the environment\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(obs)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def plot_action_counts(action_list):\n",
    "    fig, ax = plt.subplots(1,1,figsize=(6,6))\n",
    "\n",
    "    transform = lambda x : action_dict[int(x)]\n",
    "    action_series = pd.Series(action_list, name='Count').dropna()\n",
    "    data = action_series.value_counts().sort_index()\n",
    "    data /= data.sum() # normalize to probabilities\n",
    "    data.index = list(map(transform, data.index))\n",
    "    data = data.to_frame().transpose()\n",
    "    sns.barplot(data=data, palette='Spectral', ax=ax)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15faaca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(probs, i_since_r, timer_i, sigma=None, buffer=None):        \n",
    "    # autograd no inplace ops\n",
    "    if buffer is None:\n",
    "        buffer = timer_i // 2\n",
    "    n = len(probs)\n",
    "    if sigma is None:\n",
    "        sigma = 2 / n\n",
    "    noise = torch.normal(0., sigma, (1, n)) # means, stds shared, size n\n",
    "    noise = noise - noise.mean()\n",
    "    \n",
    "    scale = i_since_r / (timer_i - buffer)\n",
    "\n",
    "    probs = probs + noise * scale\n",
    "    pmin = torch.min(probs)\n",
    "    if pmin < 0:\n",
    "        probs = probs - pmin\n",
    "        probs = probs / torch.sum(probs)\n",
    "        \n",
    "    return probs\n",
    "\n",
    "def balance_lr(probs, i_since_r, timer_i, beta=.5, buffer=None):\n",
    "#     2 : 'RIGHT'\n",
    "#     3 : 'LEFT'\n",
    "#     4 : 'RIGHTFIRE'\n",
    "#     5 : 'LEFTFIRE'\n",
    "    if i_since_r < timer_i / 4:\n",
    "        return probs\n",
    "    elif i_since_r < timer_i / 2:\n",
    "        alpha = .5\n",
    "    elif i_since_r < timer_i * 3 / 4:\n",
    "        alpha = .8\n",
    "    else:\n",
    "        alpha = .99\n",
    "\n",
    "    zero_probs = torch.zeros_like(probs)\n",
    "    zero_probs[0,2] = (probs[0,3] - probs[0,2])\n",
    "    zero_probs[0,3] = (probs[0,2] - probs[0,3])\n",
    "    \n",
    "    zero_probs[0,4] = (probs[0,5] - probs[0,4])\n",
    "    zero_probs[0,5] = (probs[0,4] - probs[0,5])\n",
    "    zero_probs = zero_probs * alpha * beta / 2\n",
    "    \n",
    "    probs = probs + zero_probs\n",
    "    with torch.no_grad():\n",
    "        assert torch.sum(probs).round(decimals=3) == 1, torch.sum(probs)\n",
    "    return probs\n",
    "\n",
    "def standardize(x):\n",
    "    eps = np.finfo(np.float64).eps.item()\n",
    "    x = (x - x.mean()) / (x.std() + eps)\n",
    "    return x\n",
    "\n",
    "def balance_all(probs, i_since_r, timer_i, beta=2):\n",
    "    probs = probs + 2 * i_since_r / timer_i\n",
    "    probs = softmax = nn.Softmax(dim=-1)(probs)\n",
    "    return probs\n",
    "    \n",
    "def modify_reward(action, reward, info, prev_lives, i_since_r, timer_i=1000, reward_dict=None):\n",
    "    if reward_dict is None:\n",
    "        reward_dict = {\n",
    "            'life_penalty' : 15,\n",
    "            'nofire_penalty' : .1,\n",
    "            'comeback_reward' : 10,\n",
    "        }\n",
    "    if info['lives'] < prev_lives:\n",
    "        reward += reward_dict['life_penalty']\n",
    "    if reward <= 0 and action in [1,4,5]:\n",
    "        reward += reward_dict['nofire_penalty']\n",
    "    if reward > 0 and i_since_r > timer_i / 2:\n",
    "        reward += reward_dict['comeback_reward']\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "829d65ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_probs(probs, i_since_r, timer_i=1000, corner_correct=True):\n",
    "    truncated = i_since_r > timer_i\n",
    "        \n",
    "    if corner_correct: # heavily biases agent from getting 'stuck' in corner\n",
    "        probs = add_noise(probs, i_since_r, timer_i)\n",
    "        probs = balance_lr(probs, i_since_r, timer_i)\n",
    "    else:\n",
    "        probs = balance_all(probs, i_since_r, timer_i)\n",
    "\n",
    "    if torch.round(torch.sum(probs), decimals=4) != 1:\n",
    "        raise ValueError('Probs do not sum to 1')\n",
    "        \n",
    "    return probs, truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc47b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBS_SHAPE = (210, 160)\n",
    "action_dict = {\n",
    "    0 : 'NOOP',\n",
    "    1 : 'FIRE',\n",
    "    2 : 'RIGHT',\n",
    "    3 : 'LEFT',\n",
    "    4 : 'RIGHTFIRE',\n",
    "    5 : 'LEFTFIRE'\n",
    "}\n",
    "ACTIONS = [0,1,2,3,4,5] # modify to limit available actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edb49d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearReinforcement(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        hidden_dim, \n",
    "        output_dim, \n",
    "        gamma=.99, \n",
    "        lr=.08,\n",
    "        xmin=26, \n",
    "        xmax=196, \n",
    "        ymin=10, \n",
    "        ymax=144,\n",
    "        downsample = 'max_pool',\n",
    "    ):\n",
    "        super(LinearReinforcement, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.xmin = xmin\n",
    "        self.xmax = xmax\n",
    "        self.ymin = ymin\n",
    "        self.ymax = ymax\n",
    "        self.downsample = downsample\n",
    "        \n",
    "        # preprocess args\n",
    "        input_shape = (xmax - xmin, ymax - ymin)\n",
    "        input_dim = np.prod(input_shape)\n",
    "        if downsample == 'horizontal':\n",
    "            input_dim //= 2\n",
    "        elif downsample == 'max_pool':\n",
    "            input_dim //= 4\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.log_probss = list()\n",
    "        self.rewards = list()\n",
    "        self.episode_losses = list()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.optimizer = torch.optim.RMSprop(self.parameters(), lr=lr)\n",
    "        return\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 2\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer2(x)\n",
    "        probs = self.softmax(x)\n",
    "        return probs\n",
    "\n",
    "    def discount_rewards(self):\n",
    "        running_add = 0\n",
    "        discounted_rewards = list()\n",
    "        for reward in self.rewards:\n",
    "            running_add = running_add * self.gamma + reward\n",
    "            discounted_rewards.append(running_add)\n",
    "        discounted_rewards = torch.tensor(list(reversed(discounted_rewards)))\n",
    "        return discounted_rewards\n",
    "\n",
    "    def reward_loss(self):\n",
    "        assert len(self.rewards)\n",
    "\n",
    "        discounted_rewards = self.discount_rewards()\n",
    "        discounted_rewards = standardize(discounted_rewards)\n",
    "        log_probs = -torch.cat(self.log_probss)\n",
    "        \n",
    "        loss = torch.sum(torch.mul(log_probs, discounted_rewards))\n",
    "        return loss\n",
    "    \n",
    "    def episode_backward(self):\n",
    "        assert len(self.log_probss)\n",
    "        episode_loss = self.reward_loss()\n",
    "        self.episode_losses.append(episode_loss.unsqueeze(0))\n",
    "        \n",
    "        model.log_probss.clear()\n",
    "        model.rewards.clear()\n",
    "        return model\n",
    "    \n",
    "    def batch_backward(self):\n",
    "        assert len(self.episode_losses)\n",
    "        self.optimizer.zero_grad() # make sure this shouldn't go in reward_loss\n",
    "        loss = torch.sum(torch.cat(self.episode_losses))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.episode_losses.clear()\n",
    "        return model\n",
    "    \n",
    "    def clear(self):\n",
    "        self.log_probss.clear()\n",
    "        self.rewards.clear()\n",
    "        return\n",
    "    \n",
    "    def save(self, save_path=save_path):\n",
    "        if save_path is None:\n",
    "            save_path = f'{self.linear_layer.out_features}n_model.pt'\n",
    "        torch.save(self, save_path)\n",
    "        \n",
    "    def preprocess(self, x):\n",
    "        assert obs.shape == (210, 160)\n",
    "        x = x[self.xmin:self.xmax,self.ymin:self.ymax]\n",
    "        if self.downsample == 'horizontal':\n",
    "            x = x[::2,:]\n",
    "        x[x == 144] = 0 # erase background (background type 1)\n",
    "        x[x == 109] = 0 # erase background (background type 2)\n",
    "        x[x != 0] = 1 # everything else to 1\n",
    "        if self.downsample == 'max_pool':\n",
    "            # ideally downsampling would be done before changing values in place, but this way the background is ignored easily\n",
    "            x = block_reduce(x, (2, 2), np.amax)\n",
    "        x = torch.from_numpy(x.ravel()).double().unsqueeze(dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc401574",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearReinforcement(64, 6).double()\n",
    "if resume:\n",
    "#     model.load_state_dict(torch.load(save_path))\n",
    "#     model.eval()\n",
    "    model = torch.load(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a94cf0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_batch_bar(rewards, batch_size):\n",
    "    assert not len(rewards) % batch_size, len(rewards)\n",
    "    n_splits = len(rewards) // batch_size\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            f'Batch {i}' : rewards[i:i+batch_size]\n",
    "            for i\n",
    "            in range(n_splits)\n",
    "        }\n",
    "    )\n",
    "    plt.figure(figsize=(16,10))\n",
    "    sns.barplot(data=data, palette='mako')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc7f00dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500                          \n",
      "1000                         \n",
      "1500                          \n",
      "1596                          \n",
      "Episode 0 of 10240                \n",
      "\n",
      "500                          \n",
      "1000                         \n",
      "1338                          \n",
      "Episode 1 of 10240                \n",
      "\n",
      "500                          \n",
      "1000                         \n",
      "1500                          \n",
      "2000                          \n",
      "2500                          \n",
      "3000                          \n",
      "3500                          \n",
      "4000                          \n",
      "4297                          \n",
      "Episode 2 of 10240                \n",
      "\n",
      "500                          \n",
      "934                          \n",
      "Episode 3 of 10240                \n",
      "\n",
      "500                          \n",
      "701                          \n",
      "Episode 4 of 10240                \n",
      "\n",
      "500                          \n",
      "1000                         \n",
      "1500                          \n",
      "2000                          \n",
      "2500                          \n",
      "2697                          \n",
      "Episode 5 of 10240                \n",
      "\n",
      "500                          \n",
      "1000                         \n",
      "1500                          \n",
      "1521                          \n",
      "Episode 6 of 10240                \n",
      "\n",
      "500                          \n",
      "915                          \n",
      "Episode 7 of 10240                \n",
      "\n",
      "500                          \n",
      "1000                         \n",
      "1500                          \n",
      "1862                          \n",
      "Episode 8 of 10240                \n",
      "\n",
      "500                          \n",
      "936                          \n",
      "Episode 9 of 10240                \n",
      "\n",
      "500                          \n",
      "549                          \n",
      "Episode 10 of 10240                \n",
      "\n",
      "500                          \n",
      "1000                         \n",
      "1500                          \n",
      "2000                          \n",
      "2500                          \n",
      "2667                          \n",
      "Episode 11 of 10240                \n",
      "\n",
      "500                          \n",
      "755                          \n",
      "Episode 12 of 10240                \n",
      "\n",
      "500                          \n",
      "1000                         \n",
      "1500                          \n",
      "1756                          \n",
      "Episode 13 of 10240                \n",
      "\n",
      "500                          \n",
      "721                          \n",
      "Episode 14 of 10240                \n",
      "\n",
      "500                          \n",
      "1000                         \n",
      "1500                          \n",
      "2000                          \n",
      "2500                          \n",
      "3000                          \n",
      "3500                          \n",
      "4000                          \n",
      "4500                          \n",
      "4673                          \n",
      "Episode 15 of 10240                \n",
      "\n",
      "369                          \n",
      "Episode 16 of 10240                \n",
      "\n",
      "317                          \n",
      "Episode 17 of 10240                \n",
      "\n",
      "500                          \n",
      "511                          \n",
      "Episode 18 of 10240                \n",
      "\n",
      "500                          \n",
      "947                          \n",
      "Episode 19 of 10240                \n",
      "\n",
      "500                          \n",
      "567                          \n",
      "Episode 20 of 10240                \n",
      "\n",
      "500                          \n",
      "597                          \n",
      "Episode 21 of 10240                \n",
      "\n",
      "465                          \n",
      "Episode 22 of 10240                \n",
      "\n",
      "500                          \n",
      "1000                         \n",
      "1500                          \n",
      "1821                          \n",
      "Episode 23 of 10240                \n",
      "\n",
      "500                          \n",
      "1000                         \n",
      "1500                          \n",
      "2000                          \n",
      "2500                          \n",
      "3000                          \n",
      "3500                          \n",
      "4000                          \n",
      "4500                          \n",
      "5000                          \n",
      "5500                          \n",
      "6000                          \n",
      "6500                          \n",
      "7000                          \n",
      "7500                          \n",
      "8000                          \n",
      "8500                          \n",
      "9000                          \n",
      "9231                          \n",
      "Episode 24 of 10240                \n",
      "\n",
      "500                          \n",
      "1000                         \n",
      "1500                          \n",
      "2000                          \n",
      "2500                          \n",
      "3000                          \n",
      "3039                          \n",
      "Episode 25 of 10240                \n",
      "\n",
      "500                          \n",
      "1000                         \n",
      "1311                          \n",
      "Episode 26 of 10240                \n",
      "\n",
      "500                          \n",
      "725                          \n",
      "Episode 27 of 10240                \n",
      "\n",
      "500                          \n",
      "1000                         \n",
      "1500                          \n",
      "2000                          \n",
      "2500                          \n",
      "3000                          \n",
      "3500                          \n",
      "3651                          \n",
      "Episode 28 of 10240                \n",
      "\n",
      "500                          \n",
      "1000                         \n",
      "1500                          \n",
      "2000                          \n",
      "2500                          \n",
      "3000                          \n",
      "3007                          \n",
      "Episode 29 of 10240                \n",
      "\n",
      "500                          \n",
      "1000                         \n",
      "1500                          \n",
      "2000                          \n",
      "2500                          \n",
      "2565                          \n",
      "Episode 30 of 10240                \n",
      "\n",
      "500                          \n",
      "1000                         \n",
      "1500                          \n",
      "2000                          \n",
      "2481                          \n",
      "Episode 31 of 10240                \n",
      "\n",
      "\r"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "59555",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 86\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_grad \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m episode_number \u001b[38;5;241m%\u001b[39m batch_size:           \n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m record_rewards:\n\u001b[1;32m---> 86\u001b[0m         \u001b[43mplot_batch_bar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreward_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m         plot_rewards(reward_list, window_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m     88\u001b[0m     model\u001b[38;5;241m.\u001b[39mbatch_backward()\n",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m, in \u001b[0;36mplot_batch_bar\u001b[1;34m(rewards, batch_size)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_batch_bar\u001b[39m(rewards, batch_size):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rewards) \u001b[38;5;241m%\u001b[39m batch_size, \u001b[38;5;28mlen\u001b[39m(rewards)\n\u001b[0;32m      3\u001b[0m     n_splits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(rewards) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size\n\u001b[0;32m      4\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[0;32m      5\u001b[0m         {\n\u001b[0;32m      6\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m : rewards[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m         }\n\u001b[0;32m     10\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: 59555"
     ]
    }
   ],
   "source": [
    "n_episodes = 2560 * 4\n",
    "batch_size = 64\n",
    "print_every = 500\n",
    "apply_stop = False\n",
    "\n",
    "prev_x = None # used in computing the difference frame\n",
    "reward_sum = 0\n",
    "adj_reward_sum = 0\n",
    "last_i = 0\n",
    "env = gym.make(\n",
    "    'ALE/DemonAttack-v5', # alternate games can be chosen here \n",
    "    obs_type='grayscale', # saves RGB preprocessing reduction\n",
    "    render_mode='human' if render else None, # rendering shows popup but limits training speed\n",
    ")\n",
    "\n",
    "if record_actions:\n",
    "    action_list = list()\n",
    "if record_rewards:\n",
    "    reward_list = list()\n",
    "if record_probs:\n",
    "    prob_list = list()\n",
    "\n",
    "episode_number = 0\n",
    "prev_lives = 0\n",
    "i = 0\n",
    "e_ = 0\n",
    "\n",
    "obs, info = env.reset()\n",
    "while episode_number <= n_episodes:\n",
    "    curr_x = model.preprocess(obs)\n",
    "    x = curr_x - prev_x if prev_x is not None else torch.zeros((1, model.input_dim)).double() # only monitor change between frames\n",
    "    prev_x = curr_x\n",
    "    assert x.dim() == 2, x.dim()\n",
    "    model_probs = model(x)\n",
    "    i_since_r = i - last_i\n",
    "    probs, ptruncated = process_probs(model_probs, i_since_r, timer_i=timer_i, corner_correct=corner_correct)\n",
    "    probs = (probs + torch.tensor([1/len(probs)] * len(probs)) * e_)\n",
    "    probs = probs / probs.sum()\n",
    "    \n",
    "    if record_probs:\n",
    "        prob_list.append(probs)\n",
    "\n",
    "    m = torch.distributions.Categorical(probs)\n",
    "    action = m.sample() \n",
    "\n",
    "    log_probs = m.log_prob(action)\n",
    "    model.log_probss.append(log_probs)\n",
    "    action = action.item()\n",
    "\n",
    "    if record_actions or plot_action_every:\n",
    "        action_list.append(action)\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env.step(action) # step returns all other relevant information \n",
    "    if apply_stop:\n",
    "        truncated = truncated or ptruncated\n",
    "    elif i - last_i > 100000:\n",
    "        truncated = True\n",
    "    \n",
    "    if reward > 0: # reset the iterations since last reward if reward is accrued\n",
    "        last_i = i\n",
    "        e_ = 0\n",
    "    else:\n",
    "        e_ += .00008\n",
    "        \n",
    "    reward_sum += reward # total round reward incremented\n",
    "    \n",
    "    adj_reward = modify_reward(action, reward, info, prev_lives, i_since_r, timer_i=timer_i) # adjusted reward may better lead agent toward short term optimums\n",
    "    prev_lives = info['lives']\n",
    "    adj_reward_sum += adj_reward\n",
    "    model.rewards.append(adj_reward)\n",
    "    ######################################################\n",
    "    \n",
    "        \n",
    "    if terminated: # an episode finished\n",
    "        if record_rewards:\n",
    "            reward_list.append(reward_sum)\n",
    "        print(f'\\nEpisode {episode_number} of {n_episodes}                \\n\\n', end='\\r')\n",
    "        \n",
    "        episode_number += 1\n",
    "\n",
    "        # Finish The Episode.save\n",
    "        model.episode_backward()\n",
    "                \n",
    "        # Finish the Batch\n",
    "        if not no_grad and not episode_number % batch_size:           \n",
    "            if record_rewards:\n",
    "                plot_batch_bar(reward_list, batch_size)\n",
    "                plot_rewards(reward_list, window_size=batch_size)\n",
    "            model.batch_backward()\n",
    "            model.save(save_path=save_path)\n",
    "            \n",
    "        if plot_action_every and not episode_number % plot_action_every:\n",
    "            plot_action_counts(action_list)\n",
    "            action_list.clear()\n",
    "\n",
    "    elif truncated: # an episode terminated unexpectedly, shouldn't maintain results\n",
    "        model.episode_losses.clear()\n",
    "        model.log_probss.clear()\n",
    "        model.rewards.clear()       \n",
    "    elif print_every and not i % print_every:\n",
    "        print(i)\n",
    "    else:\n",
    "        print(i, end='                          \\r')\n",
    "        \n",
    "    if terminated or truncated:\n",
    "        reward_sum = 0\n",
    "        adj_reward_sum = 0\n",
    "        \n",
    "        obs, info = env.reset()\n",
    "        prev_x = None\n",
    "        i = 0\n",
    "        prev_lives = 3 # for new episode adjustment\n",
    "        if record_probs:\n",
    "            plot_probs(prob_list, dpi=80)\n",
    "            prob_list.clear()\n",
    "    i += 1\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111d293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if record_probs:\n",
    "    plot_probs(prob_list)\n",
    "if record_rewards:\n",
    "    plot_rewards(reward_list, window_size=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
