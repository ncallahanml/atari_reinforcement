{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df726742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install \"gymnasium[atari]\"\n",
    "# !python -m pip install \"gymnasium[accept-rom-license, atari]\"\n",
    "# !pip install shimmy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "567e4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import ale_py\n",
    "import shimmy\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from gym import wrappers\n",
    "from skimage.measure import block_reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b7597b",
   "metadata": {},
   "source": [
    "| **Value** | **Meaning** |\n",
    "|:---------:|:-----------:|\n",
    "| 0 | NOOP |\n",
    "| 1 | FIRE |\n",
    "| 2 | RIGHT |\n",
    "| 3 | LEFT |\n",
    "| 4 | RIGHTFIRE |\n",
    "| 5 | LEFTFIRE |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baec494",
   "metadata": {},
   "source": [
    "# General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28496867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_obs(obs):\n",
    "    \"\"\" \n",
    "    Simple display of image observation \n",
    "    \n",
    "    Args:\n",
    "    `obs` : np.ndarray\n",
    "    - Observation from the environment\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(obs)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f502cc94",
   "metadata": {},
   "source": [
    "# Policy Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15a4713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards):\n",
    "    \"\"\" \n",
    "    Take 1D array of rewards and compute discounted version\n",
    "    Most recent action has the greatest weight \n",
    "    \n",
    "    Args:\n",
    "    `rewards` : np.ndarray\n",
    "    - Observed rewards over time\n",
    "    - ndim : 1\n",
    "    \"\"\"\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, rewards.size)):\n",
    "        running_add = running_add * gamma + rewards[t]\n",
    "        discounted_rewards[t] = running_add\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ffbd68",
   "metadata": {},
   "source": [
    "# Run Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e7513bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64      # used to perform a RMS prop param update every batch_size steps\n",
    "learning_rate = 1e-2 # learning rate used in RMS prop\n",
    "gamma = 0.99         # discount factor for reward\n",
    "decay_rate = 0.99    # decay factor for RMSProp leaky sum of grad^2\n",
    "\n",
    "resume = False # resume training from previous checkpoint (from save.p  file)?\n",
    "render = False # render video output?\n",
    "print_ = False # print each observation\n",
    "show = False\n",
    "zero_grad = True\n",
    "corner_correct = True\n",
    "\n",
    "timer_i = 1000 # number of iterations without reward before noise is intentionally greater than signal\n",
    "\n",
    "record_probs = True\n",
    "record_rewards = True\n",
    "record_eps_iters = True\n",
    "save_path = 'model.joblib'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ead628",
   "metadata": {},
   "source": [
    "# Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc47b8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: (170, 130)\n",
      "Input Dimensionality: 11050\n"
     ]
    }
   ],
   "source": [
    "OBS_SHAPE = (210, 160)\n",
    "XMIN = 26\n",
    "XMAX = 196\n",
    "YMIN = 14\n",
    "YMAX = 144\n",
    "SHAPE = (XMAX - XMIN, YMAX - YMIN)\n",
    "downsample = 'horizontal'\n",
    "if downsample == 'max_pool':\n",
    "    dim = np.prod(SHAPE) // 4\n",
    "elif downsample == 'horizontal':\n",
    "    dim = np.prod(SHAPE) // 2\n",
    "else:\n",
    "    dim = np.prod(SHAPE)\n",
    "    \n",
    "action_dict = {\n",
    "    0 : 'NOOP',\n",
    "    1 : 'FIRE',\n",
    "    2 : 'RIGHT',\n",
    "    3 : 'LEFT',\n",
    "    4 : 'RIGHTFIRE',\n",
    "    5 : 'LEFTFIRE'\n",
    "}\n",
    "ACTIONS = [0,1,2,3,4,5]\n",
    "N_CLASSES = len(ACTIONS)\n",
    "\n",
    "print('Input Shape:', SHAPE)\n",
    "print('Input Dimensionality:', dim)\n",
    "\n",
    "def preprocess(obs, downsample='horizontal', xmin=26, xmax=196, ymin=10, ymax=144):\n",
    "    assert obs.shape == (210, 160)\n",
    "    if downsample not in ['horizontal','max_pool']:\n",
    "        raise ValueError(f'Invalid downsample operation {downsample}')\n",
    "    I = obs[xmin:xmax,ymin:ymax]\n",
    "    if downsample == 'horizontal':\n",
    "        I = I[::2,:]\n",
    "    I[I == 144] = 0 # erase background 1\n",
    "    I[I == 109] = 0 # erase background 2\n",
    "    I[I != 0] = 1 # everything else is kept\n",
    "    if downsample == 'max_pool':\n",
    "        # ideally downsampling would be done before changing values in place, but this way the background is ignored easily\n",
    "        I = block_reduce(I, (2, 2), np.amax)\n",
    "    return I.astype(np.float32).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c2092",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32e6e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(arr):\n",
    "    \"\"\" Standardizes input array `arr` to unit variance and mean of 0 \"\"\"\n",
    "    arr -= np.mean(arr)\n",
    "    arr /= np.std(arr)\n",
    "    return arr\n",
    "\n",
    "def lrelu(x, alpha=.03):\n",
    "    \"\"\"\n",
    "    Leaky ReLU activation function\n",
    "    \n",
    "    Args:\n",
    "    `x` : np.ndarray\n",
    "    - ndim doesn't matter\n",
    "    - Operated on in place\n",
    "    \n",
    "    `alpha` : float\n",
    "    - Attenuation coefficient for negative values\n",
    "    - Value of 0 makes this equivalent to regular ReLU\n",
    "    \"\"\"\n",
    "    x[x < 0] *= alpha\n",
    "    return\n",
    "    \n",
    "def dropout(x, frac=.2):\n",
    "    \"\"\"\n",
    "    Dropout to limit overfitting\n",
    "    Selects weights from the 0th dimension of the passed array to be reset to 0\n",
    "    \n",
    "    Args:\n",
    "    `x` : np.ndarray\n",
    "    - Only 0th dimension is operated on\n",
    "    \n",
    "    `frac` : float\n",
    "    - Must be less than 1, greater than 0\n",
    "    - Determines number of indices to reset\n",
    "    - Higher number means more dropout, more training, less overfitting\n",
    "    \"\"\"\n",
    "    if frac:\n",
    "        drop_indices = np.random.choice(x.shape[0], size=int(x.shape[0]*frac), replace=False)\n",
    "        x[drop_indices] = 0\n",
    "    return\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Performs softmax on hidden logits\n",
    "    All output values scaled between 0 and 1, sum to 1\n",
    "    Vector adjustment of sigmoid\n",
    "    \n",
    "    Args:\n",
    "    `x` : np.ndarray\n",
    "    - ndim : 1\n",
    "    - Output will be same shape, but normalized\n",
    "    \n",
    "    Returns:\n",
    "    `x_softmax` : np.ndarray\n",
    "    - Same shape as `x` but scaled as probabilities\n",
    "    \"\"\"\n",
    "    x_exp = np.exp(x - np.max(x))\n",
    "    x_sum = np.sum(x_exp)\n",
    "    x_softmax = np.divide(x_exp, x_sum) #normalize\n",
    "    assert round(np.sum(x_softmax), 5) == 1, x_softmax\n",
    "    return x_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a55e7f",
   "metadata": {},
   "source": [
    "## Three Layer Network\n",
    "- Single hidden layer + input/output layers\n",
    "- Leaky Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6e73ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triple_layer(n_neurons=64, n_classes=6, dim=6000):\n",
    "    middle_n = n_neurons // 2\n",
    "    model = {\n",
    "        'Layer1' : np.random.randn(n_neurons, middle_n) / np.sqrt(middle_n),\n",
    "        'Layer2' : np.random.randn(middle_n, dim) / np.sqrt(dim),\n",
    "        'Layer3' : np.random.standard_normal((n_classes, n_neurons)) / np.sqrt(n_neurons),\n",
    "    }\n",
    "    return model\n",
    "   \n",
    "def policy_triple_forward(x, alpha=.03, frac=.2):\n",
    "    x = np.dot(model['Layer1'], x)\n",
    "    lrelu(x, alpha=alpha)\n",
    "    dropout(x, frac=frac)\n",
    "\n",
    "    alpha /= 2\n",
    "    frac /= 2\n",
    "    \n",
    "    x = np.dot(model['Layer2'], x)\n",
    "    lrelu(x, alpha=alpha)\n",
    "    dropout(x, frac=frac)\n",
    "    \n",
    "    logp = np.dot(model['Layer3'], x)\n",
    "    p = softmax(logp)\n",
    "    return p, x\n",
    "\n",
    "def policy_triple_backward(eph, epx, epdlogp):\n",
    "    dl3 = np.dot(eph.transpose(), epdlogp).ravel()\n",
    "    dx = np.outer(epdlogp, model['Layer3'])\n",
    "    dx[eph <= 0] = 0\n",
    "    \n",
    "    dl2 = np.outer(dx, model['Layer2'])\n",
    "    dh[eph <= 0] = 0\n",
    "    \n",
    "    dW1 = np.dot(dh.transpose(), epx)\n",
    "    model = {\n",
    "        'Layer1' : dl1, \n",
    "        'Layer2' : dl2, \n",
    "        'Layer3' : dl3,\n",
    "    }\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07401242",
   "metadata": {},
   "source": [
    "## Two Layer Network\n",
    "- Input and output layers\n",
    "- relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2c649f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(hidden_size=128, n_classes=6):\n",
    "    model = {\n",
    "        'W1' : np.random.randn(hidden_size, dim) / np.sqrt(dim),\n",
    "        'W2' : np.random.standard_normal((n_classes, hidden_size)) / np.sqrt(hidden_size),\n",
    "    }\n",
    "    return model\n",
    "\n",
    "def policy_forward(x, alpha=.03):\n",
    "    h = np.dot(model['W1'], x)\n",
    "    lrelu(h, alpha=alpha)\n",
    "    logp = np.dot(model['W2'], h)\n",
    "    probs = softmax(logp)\n",
    "    return probs, h\n",
    "\n",
    "def policy_backward(eph, epx, epdlogp):\n",
    "    dW2 = np.dot(eph.transpose(), epdlogp).transpose()\n",
    "    dh = np.dot(epdlogp, model['W2'])\n",
    "    \n",
    "    dh[eph < 0] = 0\n",
    "    dW1 = np.dot(dh.transpose(), epx)\n",
    "    model = {\n",
    "        'W1' : dW1,\n",
    "        'W2' : dW2,\n",
    "    }\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "996789b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if resume and os.path.exists(save_path):\n",
    "    model = joblib.load(save_path)\n",
    "else:\n",
    "    model = network()\n",
    "\n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() }\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8576d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_x = None\n",
    "xs, hs, dlogps, drs = list(), list(), list(), list()\n",
    "running_reward = None\n",
    "\n",
    "reward_sum = 0\n",
    "adj_reward_sum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15faaca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(probs, i, i_since_r, timer_i, buffer=None, print_=False):\n",
    "    if buffer is None:\n",
    "        buffer = timer_i // 2\n",
    "    n = len(probs)\n",
    "    sigma = 2 / n\n",
    "    noise = np.random.normal(0, sigma, size=n)\n",
    "    noise = noise - np.mean(noise)\n",
    "    \n",
    "    scale = i_since_r / (timer_i - buffer)\n",
    "    noise = noise * scale\n",
    "    assert not round(np.mean(noise), 3), noise\n",
    "    if print_ and not i % 100:\n",
    "        print(probs)\n",
    "        print(noise)\n",
    "    new_probs = probs + noise\n",
    "    pmin = np.amin(new_probs)\n",
    "    if pmin < 0:\n",
    "        new_probs -= pmin\n",
    "        new_probs /= np.sum(new_probs)\n",
    "    return new_probs\n",
    "\n",
    "def balance_lr(probs, i_since_r, timer_i, buffer=None):\n",
    "#    ACTIONS = [NOOP,1,2,3,4,5]\n",
    "    if i_since_r < timer_i // 4:\n",
    "        pass\n",
    "    elif i_since_r < timer_i // 2:\n",
    "        equal_n = (probs[2] + probs[3]) / 2\n",
    "        equal_y = (probs[4] + probs[5]) / 2\n",
    "        probs[2] = equal_n\n",
    "        probs[3] = equal_n\n",
    "        probs[4] = equal_y\n",
    "        probs[5] = equal_y\n",
    "    elif i_since_r < 3 * timer_i // 4:\n",
    "        probs[2], probs[3] = probs[3], probs[2]\n",
    "        probs[4], probs[5] = probs[5], probs[4]\n",
    "    return probs\n",
    "    \n",
    "def modify_reward(action, reward, info, prev_lives):\n",
    "    if info['lives'] < prev_lives:\n",
    "        reward -= 15\n",
    "    if reward <= 0 and action in [1,4,5]:\n",
    "        reward -= 1\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7f00dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:42: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (210, 160)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 37 of 2000 episodes                \r"
     ]
    }
   ],
   "source": [
    "n_episodes = 2000\n",
    "last_i = 0\n",
    "env = gym.make(\n",
    "    'ALE/DemonAttack-v5', # alternate games can be chosen here \n",
    "    obs_type='grayscale', # saves RGB preprocessing reduction\n",
    "    render_mode='human' if render else None, # rendering shows popup but limits training speed\n",
    ")\n",
    "        \n",
    "if record_rewards:\n",
    "    reward_list = list()\n",
    "if record_probs:\n",
    "    prob_list = list()\n",
    "if record_eps_iters:\n",
    "    eps_iters_list = list()\n",
    "\n",
    "obs, info = env.reset()\n",
    "\n",
    "episode_number = 0\n",
    "prev_lives = 0\n",
    "i = 0\n",
    "\n",
    "while episode_number <= n_episodes:\n",
    "    curr_x = preprocess(obs, downsample=downsample, xmin=XMIN, xmax=XMAX, ymin=YMIN, ymax=YMAX)\n",
    "    x = curr_x - prev_x if prev_x is not None else np.zeros(dim) # only monitor change between frames\n",
    "    prev_x = curr_x\n",
    "\n",
    "    # forward the policy network and sample an action from the returned probability\n",
    "    probs, h = policy_forward(x)\n",
    "    \n",
    "    i_since_r = i - last_i\n",
    "    if i_since_r > timer_i:\n",
    "        terminated = True\n",
    "        truncated = False\n",
    "        print('Timer causing reset               ')\n",
    "        last_i = i\n",
    "\n",
    "    xs.append(x) # observation\n",
    "    hs.append(h) # hidden state\n",
    "\n",
    "    if corner_correct: # heavily biases agent from getting 'stuck' in corner\n",
    "        probs = add_noise(probs, i, i_since_r, timer_i)\n",
    "        probs = balance_lr(probs, i_since_r, timer_i)\n",
    "\n",
    "    try:\n",
    "        action = np.random.choice(ACTIONS, p=probs) # RANDOMLY choose one with probability weight based on forward pass expectations\n",
    "    except Exception as e:\n",
    "        print(probs)\n",
    "        raise(e)\n",
    "        \n",
    "    if record_probs:\n",
    "        prob_list.append(probs)\n",
    "\n",
    "    y = np.zeros_like(probs) # create zeroed probability array\n",
    "    y[ACTIONS.index(action)] = 1 # assign all probability to single element (chosen action)\n",
    "    dlogps.append(y - probs) \n",
    "    \n",
    "    #####################################################\n",
    "    prev_lives = info['lives'] # lives not available through general step return\n",
    "    obs, reward, terminated, truncated, info = env.step(action) # step returns all other relevant information \n",
    "    if reward > 0: # reset the iterations since last reward if reward is accrued\n",
    "        last_i = i\n",
    "\n",
    "    reward_sum += reward # total round reward incremented\n",
    "    adj_reward = modify_reward(action, reward, info, prev_lives) # adjusted reward may better lead agent toward short term optimums\n",
    "    adj_reward_sum += adj_reward\n",
    "    drs.append(adj_reward) # record reward\n",
    "    ######################################################\n",
    "\n",
    "    if terminated: # an episode finished\n",
    "        episode_number += 1\n",
    "#         print(f'Episode: {episode_number}              ')\n",
    "\n",
    "        if record_rewards:\n",
    "            reward_list.append(reward_sum)\n",
    "        if record_eps_iters:\n",
    "            eps_iters_list.append(i)\n",
    "        \n",
    "        # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "        if not zero_grad:\n",
    "            epx = np.vstack(xs)\n",
    "            eph = np.vstack(hs)\n",
    "            epdlogp = np.vstack(dlogps)\n",
    "            epr = np.vstack(drs)\n",
    "            for lis in [xs, hs, dlogps, drs]:\n",
    "                lis.clear()\n",
    "            \n",
    "            discounted_epr = discount_rewards(epr)\n",
    "            # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "            \n",
    "            discounted_epr = standardize(discounted_epr)\n",
    "            epdlogp *= discounted_epr # modulate the gradient with advantage (Policy Grad magic happens right here.)\n",
    "\n",
    "            grad = policy_backward(eph, epx, epdlogp)\n",
    "            for layer in model.keys():\n",
    "                grad_buffer[layer] += grad[layer] # accumulate grad over batch\n",
    "\n",
    "            # perform rmsprop parameter update every batch_size episodes\n",
    "            if episode_number % batch_size == 0:\n",
    "                for layer, weights in model.items():\n",
    "                    g = grad_buffer[layer] # gradient\n",
    "                    rmsprop_cache[layer] = decay_rate * rmsprop_cache[layer] + (1 - decay_rate) * g**2\n",
    "                    model[layer] += learning_rate * g / (np.sqrt(rmsprop_cache[layer]) + 1e-5)\n",
    "                    grad_buffer[layer] = np.zeros_like(weights) # reset batch gradient buffer\n",
    "                print('Backward Policy Applied                ')\n",
    "            running_reward = adj_reward_sum if running_reward is None else running_reward * 0.99 + adj_reward_sum * 0.01\n",
    "        \n",
    "        reward_sum = 0 # reset all totals\n",
    "        adj_reward_sum = 0\n",
    "        \n",
    "        obs, info = env.reset() # reset env\n",
    "        prev_x = None\n",
    "    elif truncated: # an episode terminated unexpectedly, shouldn't maintain results\n",
    "        for lis in [xs, hs, dlogps, drs]: # empty tracking\n",
    "            lis.clear()\n",
    "        \n",
    "        reward_sum = 0\n",
    "        adj_reward_sum = 0\n",
    "        \n",
    "        obs, info = env.reset()\n",
    "        prev_x = None\n",
    "        \n",
    "    if not i % 100:\n",
    "#         print(f'{round((i/iters)*100, 3)}% complete                ', end='\\r')\n",
    "        print(f'Episode {episode_number} of {n_episodes} episodes                ', end='\\r')\n",
    "        joblib.dump(model, save_path)\n",
    "    i += 1\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299817c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probs(prob_list, eps_iters_list, batch_size, step=1):\n",
    "    probs_arr = np.vstack(prob_list)\n",
    "#     plt.figure(figsize=(16,5))\n",
    "    fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(16,12), dpi=200, sharex=True, sharey=True)\n",
    "    fig.suptitle('Single Episode Action Probabilities')\n",
    "    colors = sns.color_palette('Spectral', 7)\n",
    "    colors = colors[:3] + colors[4:]\n",
    "    for i, (ax, color) in enumerate(zip(axs.flatten(), colors)):\n",
    "        sns.lineplot(data=probs_arr[::step,i], color=color, label=action_dict[i], alpha=.7, dashes=False, ax=ax)\n",
    "#     plt.xticks(np.asarray(plt.xticks(), dtype=np.int32) * step)\n",
    "#     for i, eps_iters in enumerate(eps_iters_list):\n",
    "#         plt.axvline(eps_iters, 0, 1, color='red' if not i % batch_size else 'pink')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "if record_probs:\n",
    "    plot_probs(prob_list, eps_iters_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111d293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, window_size) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[window_size:] = ret[window_size:] - ret[:-window_size]\n",
    "    return ret[window_size - 1:] / window_size\n",
    "\n",
    "def plot_rewards(reward_list, window_size=10):\n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.title('Rewards Over Time')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.xlabel('Episode Number')\n",
    "    x = np.arange(0, len(reward_list), 1)\n",
    "    assert len(x) == len(reward_list)\n",
    "    plt.plot(x, reward_list, color='black', linestyle='dashed', label='Reward Per Episode')\n",
    "    plt.plot(x[window_size-1:], moving_average(reward_list, window_size), color='red', label='Reward Moving Average')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "print(reward_list)\n",
    "if record_rewards:\n",
    "    plot_rewards(reward_list, window_size=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
